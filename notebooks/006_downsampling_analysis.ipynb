{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling Impact Analysis for RF-UAVNet\n",
    "\n",
    "**Objective**: Determine optimal downsampling factor for raw IQ signals while preserving discriminative temporal features.\n",
    "\n",
    "**Context**:\n",
    "- Original: 60 MHz sampling → 1.2M samples / 20ms segment\n",
    "- Current RF-UAVNet: 120× downsampling (1.2M → 10k) with only 56% accuracy\n",
    "- Hypothesis: Excessive downsampling loses critical temporal patterns (bursts, protocol signatures)\n",
    "\n",
    "**Test factors**:\n",
    "| Target samples | Factor | Effective Fs | Nyquist limit |\n",
    "|----------------|--------|--------------|---------------|\n",
    "| 1,200,000 | 1× (baseline) | 60 MHz | 30 MHz |\n",
    "| 350,000 | ~3.4× | 17.5 MHz | 8.75 MHz |\n",
    "| 150,000 | 8× | 7.5 MHz | 3.75 MHz |\n",
    "| 40,000 | 30× | 2 MHz | 1 MHz |\n",
    "| 10,000 | 120× (current) | 500 kHz | 250 kHz |\n",
    "\n",
    "**Methods compared**:\n",
    "1. Linear interpolation (current implementation)\n",
    "2. Decimation with anti-aliasing filter (scipy.signal.decimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEGMENT_MS = 20  # Segment duration (ms)\n",
    "VIZ_MS = 2  # Visualization window (ms) - shorter for detail\n",
    "N_FILES_PER_DRONE = 2  # Files per drone for analysis\n",
    "N_SEGMENTS_PER_FILE = 3  # Segments per file\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Downsampling targets to test\n",
    "DOWNSAMPLE_TARGETS = [\n",
    "    1_200_000,  # Baseline (no downsampling)\n",
    "    350_000,    # 3.4× - conservative\n",
    "    150_000,    # 8× - moderate\n",
    "    40_000,     # 30× - aggressive\n",
    "    10_000,     # 120× - current RF-UAVNet\n",
    "]\n",
    "\n",
    "# Numerical stability\n",
    "EPSILON_AMP = 1e-10\n",
    "EPSILON_POWER = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Local imports\n",
    "from dronedetect import config, data_loader, preprocessing\n",
    "\n",
    "# Output directory\n",
    "FIGURE_DIR = Path(\"../figures/006_downsampling_analysis\")\n",
    "FIGURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Sampling rate: {config.FS/1e6:.0f} MHz\")\n",
    "print(f\"Samples per 20ms segment: {int(config.FS * SEGMENT_MS / 1000):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig, filename):\n",
    "    \"\"\"Save plotly figure as PNG.\"\"\"\n",
    "    output_path = FIGURE_DIR / f\"{filename}.png\"\n",
    "    fig.write_image(output_path, width=1400, height=800)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "df = data_loader.get_cached_metadata(force_refresh=False)\n",
    "print(f\"Loaded {len(df)} files\")\n",
    "print(f\"Drones: {sorted(df['drone_code'].unique())}\")\n",
    "print(f\"States: {sorted(df['state'].unique())}\")\n",
    "\n",
    "DRONES = sorted(df['drone_code'].unique())\n",
    "STATES = sorted(df['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downsampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iq_segment(file_path, start_sample=0, duration_ms=SEGMENT_MS):\n",
    "    \"\"\"Load IQ samples from binary file for given duration.\"\"\"\n",
    "    n_samples = int(config.FS * duration_ms / 1000)\n",
    "    iq_full = data_loader.load_raw_iq(file_path)\n",
    "    return iq_full[start_sample:start_sample + n_samples]\n",
    "\n",
    "\n",
    "def downsample_interpolation(segment: np.ndarray, target_samples: int) -> np.ndarray:\n",
    "    \"\"\"Downsample via linear interpolation (current RF-UAVNet method).\n",
    "    \n",
    "    Preserves 20ms duration but reduces sample count.\n",
    "    Fast but may introduce aliasing artifacts.\n",
    "    \"\"\"\n",
    "    if len(segment) == target_samples:\n",
    "        return segment\n",
    "    \n",
    "    t_old = np.arange(len(segment))\n",
    "    t_new = np.linspace(0, len(segment) - 1, num=target_samples)\n",
    "    \n",
    "    real_down = np.interp(t_new, t_old, segment.real)\n",
    "    imag_down = np.interp(t_new, t_old, segment.imag)\n",
    "    \n",
    "    return real_down + 1j * imag_down\n",
    "\n",
    "\n",
    "def downsample_decimate(segment: np.ndarray, target_samples: int, ftype='fir') -> np.ndarray:\n",
    "    \"\"\"Downsample via decimation with anti-aliasing filter.\n",
    "    \n",
    "    Applies low-pass filter before downsampling to prevent aliasing.\n",
    "    More computationally expensive but preserves signal integrity.\n",
    "    \n",
    "    Args:\n",
    "        segment: Complex IQ signal\n",
    "        target_samples: Desired output length\n",
    "        ftype: 'fir' (default, linear phase) or 'iir' (faster, nonlinear phase)\n",
    "    \"\"\"\n",
    "    if len(segment) == target_samples:\n",
    "        return segment\n",
    "    \n",
    "    factor = len(segment) // target_samples\n",
    "    \n",
    "    if factor <= 1:\n",
    "        return segment\n",
    "    \n",
    "    # scipy.signal.decimate handles I and Q separately\n",
    "    # For large factors, apply decimation iteratively (max factor 13 per step)\n",
    "    real_dec = segment.real.copy()\n",
    "    imag_dec = segment.imag.copy()\n",
    "    \n",
    "    remaining_factor = factor\n",
    "    while remaining_factor > 1:\n",
    "        step_factor = min(remaining_factor, 13)  # scipy limit\n",
    "        real_dec = scipy_signal.decimate(real_dec, step_factor, ftype=ftype, zero_phase=True)\n",
    "        imag_dec = scipy_signal.decimate(imag_dec, step_factor, ftype=ftype, zero_phase=True)\n",
    "        remaining_factor //= step_factor\n",
    "    \n",
    "    # Final interpolation to exact target size (if needed due to rounding)\n",
    "    if len(real_dec) != target_samples:\n",
    "        t_old = np.arange(len(real_dec))\n",
    "        t_new = np.linspace(0, len(real_dec) - 1, num=target_samples)\n",
    "        real_dec = np.interp(t_new, t_old, real_dec)\n",
    "        imag_dec = np.interp(t_new, t_old, imag_dec)\n",
    "    \n",
    "    return real_dec + 1j * imag_dec\n",
    "\n",
    "\n",
    "def get_downsample_factor(original_len, target_len):\n",
    "    \"\"\"Calculate downsampling factor and effective sampling rate.\"\"\"\n",
    "    factor = original_len / target_len\n",
    "    effective_fs = config.FS / factor\n",
    "    return factor, effective_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Comparison of Downsampling Impact\n",
    "\n",
    "Visualize how different downsampling levels affect the temporal structure of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a representative file (drone with complex temporal patterns)\n",
    "# Use AIR drone in ON state (motors active = high temporal variance)\n",
    "sample_file = df[(df['drone_code'] == 'AIR') & \n",
    "                 (df['state'] == 'ON') & \n",
    "                 (df['interference'] == 'CLEAN')].iloc[0]\n",
    "\n",
    "file_path = Path(sample_file['file_path'])\n",
    "print(f\"Sample file: {file_path.name}\")\n",
    "print(f\"Drone: {sample_file['drone_code']}, State: {sample_file['state']}\")\n",
    "\n",
    "# Load full 20ms segment\n",
    "iq_baseline = load_iq_segment(file_path, start_sample=0, duration_ms=SEGMENT_MS)\n",
    "print(f\"Baseline samples: {len(iq_baseline):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downsampled versions\n",
    "downsampled_interp = {}\n",
    "downsampled_decim = {}\n",
    "\n",
    "for target in DOWNSAMPLE_TARGETS:\n",
    "    factor, eff_fs = get_downsample_factor(len(iq_baseline), target)\n",
    "    print(f\"Target {target:>10,} samples | Factor: {factor:>6.1f}x | Effective Fs: {eff_fs/1e6:>6.2f} MHz\")\n",
    "    \n",
    "    downsampled_interp[target] = downsample_interpolation(iq_baseline, target)\n",
    "    downsampled_decim[target] = downsample_decimate(iq_baseline, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison: Amplitude envelope at different downsampling levels\n",
    "# Use a 2ms window for visibility\n",
    "\n",
    "viz_samples_baseline = int(config.FS * VIZ_MS / 1000)  # 2ms = 120k samples at 60MHz\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(DOWNSAMPLE_TARGETS), cols=2,\n",
    "    subplot_titles=[f\"{t//1000}k - Interpolation\" if t >= 1000 else f\"{t} - Interpolation\" \n",
    "                    for t in DOWNSAMPLE_TARGETS for _ in range(2)][::2] +\n",
    "                   [f\"{t//1000}k - Decimation\" if t >= 1000 else f\"{t} - Decimation\" \n",
    "                    for t in DOWNSAMPLE_TARGETS for _ in range(2)][::2],\n",
    "    horizontal_spacing=0.05,\n",
    "    vertical_spacing=0.04\n",
    ")\n",
    "\n",
    "# Rebuild subplot titles correctly\n",
    "titles = []\n",
    "for t in DOWNSAMPLE_TARGETS:\n",
    "    label = f\"{t//1000}k\" if t >= 1000 else str(t)\n",
    "    titles.append(f\"{label} - Interpolation\")\n",
    "    titles.append(f\"{label} - Decimation\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(DOWNSAMPLE_TARGETS), cols=2,\n",
    "    subplot_titles=titles,\n",
    "    horizontal_spacing=0.05,\n",
    "    vertical_spacing=0.04\n",
    ")\n",
    "\n",
    "for idx, target in enumerate(DOWNSAMPLE_TARGETS):\n",
    "    row = idx + 1\n",
    "    \n",
    "    # Calculate number of samples for 2ms at this downsampling level\n",
    "    factor, eff_fs = get_downsample_factor(len(iq_baseline), target)\n",
    "    viz_samples = int(eff_fs * VIZ_MS / 1000)\n",
    "    \n",
    "    # Time axis in microseconds\n",
    "    time_axis = np.arange(viz_samples) / eff_fs * 1e6\n",
    "    \n",
    "    # Interpolation\n",
    "    sig_interp = downsampled_interp[target][:viz_samples]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_axis, y=np.abs(sig_interp), mode='lines',\n",
    "                   line=dict(width=0.5, color='blue'), showlegend=False),\n",
    "        row=row, col=1\n",
    "    )\n",
    "    \n",
    "    # Decimation\n",
    "    sig_decim = downsampled_decim[target][:viz_samples]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_axis, y=np.abs(sig_decim), mode='lines',\n",
    "                   line=dict(width=0.5, color='green'), showlegend=False),\n",
    "        row=row, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Amplitude Envelope at Different Downsampling Levels ({VIZ_MS}ms window)\",\n",
    "    height=250 * len(DOWNSAMPLE_TARGETS),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add x-axis label only to bottom row\n",
    "fig.update_xaxes(title_text=\"Time (us)\", row=len(DOWNSAMPLE_TARGETS), col=1)\n",
    "fig.update_xaxes(title_text=\"Time (us)\", row=len(DOWNSAMPLE_TARGETS), col=2)\n",
    "\n",
    "save_figure(fig, \"Amplitude_Envelope_Downsampling_Comparison\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Feature Preservation Analysis\n",
    "\n",
    "Compute temporal features at each downsampling level and measure correlation with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_phase(phase_unwrapped):\n",
    "    \"\"\"Remove linear trend from unwrapped phase.\"\"\"\n",
    "    x = np.arange(len(phase_unwrapped))\n",
    "    coeffs = np.polyfit(x, phase_unwrapped, deg=1)\n",
    "    trend = np.polyval(coeffs, x)\n",
    "    return phase_unwrapped - trend\n",
    "\n",
    "\n",
    "def compute_temporal_features(iq_segment, effective_fs=config.FS):\n",
    "    \"\"\"Extract temporal features adapted for downsampled signals.\n",
    "    \n",
    "    Args:\n",
    "        iq_segment: Complex IQ signal\n",
    "        effective_fs: Effective sampling frequency after downsampling\n",
    "    \"\"\"\n",
    "    amplitude = np.abs(iq_segment)\n",
    "    \n",
    "    # Zero-crossing rates (normalized by segment length)\n",
    "    zcr_real = np.sum(np.diff(np.sign(iq_segment.real)) != 0) / len(iq_segment)\n",
    "    zcr_imag = np.sum(np.diff(np.sign(iq_segment.imag)) != 0) / len(iq_segment)\n",
    "    \n",
    "    # SNR estimation\n",
    "    noise_floor = np.percentile(amplitude, 10)\n",
    "    signal_power = np.mean(amplitude ** 2)\n",
    "    noise_power = noise_floor ** 2\n",
    "    snr_db = 10 * np.log10(signal_power / (noise_power + EPSILON_POWER))\n",
    "    \n",
    "    # Phase dynamics\n",
    "    phase = np.angle(iq_segment)\n",
    "    phase_unwrapped = np.unwrap(phase)\n",
    "    phase_derivative = np.diff(phase_unwrapped)\n",
    "    phase_variance = np.var(phase_unwrapped)\n",
    "    phase_derivative_std = np.std(phase_derivative)\n",
    "    \n",
    "    # Instantaneous frequency (scaled by effective sampling rate)\n",
    "    inst_freq = phase_derivative * effective_fs / (2 * np.pi)\n",
    "    inst_freq_mean = np.mean(inst_freq)\n",
    "    inst_freq_std = np.std(inst_freq)\n",
    "    \n",
    "    # I/Q imbalance\n",
    "    gain_imbalance = 20 * np.log10(\n",
    "        (np.std(iq_segment.real) + EPSILON_AMP) / (np.std(iq_segment.imag) + EPSILON_AMP)\n",
    "    )\n",
    "    \n",
    "    # Autocorrelation at multiple lags (scaled by downsampling)\n",
    "    # Lag 100 at 60MHz = 1.67us; need equivalent time delay\n",
    "    base_lags_us = [0.017, 0.083, 0.167, 0.833, 1.667]  # Time delays in us\n",
    "    autocorr = {}\n",
    "    for lag_us in base_lags_us:\n",
    "        lag_samples = max(1, int(lag_us * 1e-6 * effective_fs))\n",
    "        lag_name = f\"autocorr_{lag_us:.3f}us\"\n",
    "        if len(amplitude) > lag_samples:\n",
    "            corr = np.corrcoef(amplitude[:-lag_samples], amplitude[lag_samples:])[0, 1]\n",
    "            autocorr[lag_name] = corr if not np.isnan(corr) else 0.0\n",
    "        else:\n",
    "            autocorr[lag_name] = 0.0\n",
    "    \n",
    "    # Envelope statistics\n",
    "    envelope_variance = np.var(amplitude)\n",
    "    envelope_cv = np.std(amplitude) / (np.mean(amplitude) + EPSILON_AMP)\n",
    "    \n",
    "    return {\n",
    "        'zcr_real': zcr_real,\n",
    "        'zcr_imag': zcr_imag,\n",
    "        'snr_db': snr_db,\n",
    "        'phase_variance': phase_variance,\n",
    "        'phase_derivative_std': phase_derivative_std,\n",
    "        'inst_freq_mean': inst_freq_mean,\n",
    "        'inst_freq_std': inst_freq_std,\n",
    "        'iq_gain_imbalance_db': gain_imbalance,\n",
    "        'envelope_variance': envelope_variance,\n",
    "        'envelope_cv': envelope_cv,\n",
    "        **autocorr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample files for feature analysis (stratified by drone)\n",
    "sample_files = []\n",
    "for drone in DRONES:\n",
    "    # Use ON state (highest temporal variance) and CLEAN (no interference)\n",
    "    drone_files = df[(df['drone_code'] == drone) & \n",
    "                     (df['state'] == 'ON') & \n",
    "                     (df['interference'] == 'CLEAN')]\n",
    "    if len(drone_files) >= N_FILES_PER_DRONE:\n",
    "        sample_files.append(drone_files.sample(n=N_FILES_PER_DRONE, random_state=RANDOM_STATE))\n",
    "    elif len(drone_files) > 0:\n",
    "        sample_files.append(drone_files)\n",
    "\n",
    "sample_df = pd.concat(sample_files, ignore_index=True)\n",
    "print(f\"Analyzing {len(sample_df)} files across {len(sample_df['drone_code'].unique())} drones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features at all downsampling levels\n",
    "all_features = []\n",
    "\n",
    "for row in tqdm(sample_df.itertuples(), total=len(sample_df), desc=\"Processing files\"):\n",
    "    file_path = row.file_path\n",
    "    drone_code = row.drone_code\n",
    "    \n",
    "    for seg_idx in range(N_SEGMENTS_PER_FILE):\n",
    "        try:\n",
    "            start_sample = seg_idx * int(config.FS * SEGMENT_MS / 1000)\n",
    "            iq_baseline = load_iq_segment(file_path, start_sample, SEGMENT_MS)\n",
    "            \n",
    "            for target in DOWNSAMPLE_TARGETS:\n",
    "                factor, eff_fs = get_downsample_factor(len(iq_baseline), target)\n",
    "                \n",
    "                # Both methods\n",
    "                for method_name, downsample_fn in [('interpolation', downsample_interpolation),\n",
    "                                                    ('decimation', downsample_decimate)]:\n",
    "                    iq_down = downsample_fn(iq_baseline, target)\n",
    "                    features = compute_temporal_features(iq_down, eff_fs)\n",
    "                    features['drone_code'] = drone_code\n",
    "                    features['file_path'] = file_path\n",
    "                    features['segment_idx'] = seg_idx\n",
    "                    features['target_samples'] = target\n",
    "                    features['downsample_factor'] = factor\n",
    "                    features['method'] = method_name\n",
    "                    all_features.append(features)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {file_path} seg {seg_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "print(f\"\\nExtracted {len(features_df)} feature sets\")\n",
    "print(f\"Shape: {features_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns (exclude metadata)\n",
    "feature_cols = [c for c in features_df.columns if c not in \n",
    "                ['drone_code', 'file_path', 'segment_idx', 'target_samples', \n",
    "                 'downsample_factor', 'method']]\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation with Baseline\n",
    "\n",
    "Measure how well features at each downsampling level correlate with the baseline (1.2M samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_correlations(df, baseline_target=1_200_000):\n",
    "    \"\"\"Compute correlation of each feature at each downsampling level vs baseline.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for method in df['method'].unique():\n",
    "        method_df = df[df['method'] == method]\n",
    "        \n",
    "        # Get baseline features\n",
    "        baseline_df = method_df[method_df['target_samples'] == baseline_target]\n",
    "        \n",
    "        for target in DOWNSAMPLE_TARGETS:\n",
    "            if target == baseline_target:\n",
    "                continue\n",
    "                \n",
    "            target_df = method_df[method_df['target_samples'] == target]\n",
    "            \n",
    "            # Match segments by file and segment index\n",
    "            merged = pd.merge(\n",
    "                baseline_df[['file_path', 'segment_idx'] + feature_cols],\n",
    "                target_df[['file_path', 'segment_idx'] + feature_cols],\n",
    "                on=['file_path', 'segment_idx'],\n",
    "                suffixes=('_baseline', '_target')\n",
    "            )\n",
    "            \n",
    "            for feat in feature_cols:\n",
    "                baseline_vals = merged[f'{feat}_baseline'].values\n",
    "                target_vals = merged[f'{feat}_target'].values\n",
    "                \n",
    "                # Skip if constant\n",
    "                if np.std(baseline_vals) < 1e-10 or np.std(target_vals) < 1e-10:\n",
    "                    continue\n",
    "                \n",
    "                pearson_r, _ = pearsonr(baseline_vals, target_vals)\n",
    "                spearman_r, _ = spearmanr(baseline_vals, target_vals)\n",
    "                \n",
    "                # Relative error\n",
    "                rel_error = np.mean(np.abs(target_vals - baseline_vals) / \n",
    "                                   (np.abs(baseline_vals) + EPSILON_AMP))\n",
    "                \n",
    "                results.append({\n",
    "                    'method': method,\n",
    "                    'target_samples': target,\n",
    "                    'feature': feat,\n",
    "                    'pearson_r': pearson_r,\n",
    "                    'spearman_r': spearman_r,\n",
    "                    'relative_error': rel_error\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "corr_df = compute_feature_correlations(features_df)\n",
    "print(f\"Correlation results: {len(corr_df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average correlation by downsampling level and method\n",
    "avg_corr = corr_df.groupby(['target_samples', 'method']).agg({\n",
    "    'pearson_r': 'mean',\n",
    "    'spearman_r': 'mean',\n",
    "    'relative_error': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Average Feature Correlation with Baseline (1.2M samples):\")\n",
    "print(avg_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation degradation\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Interpolation', 'Decimation'])\n",
    "\n",
    "for col_idx, method in enumerate(['interpolation', 'decimation'], 1):\n",
    "    method_corr = corr_df[corr_df['method'] == method]\n",
    "    \n",
    "    # Group by target and compute mean/std\n",
    "    grouped = method_corr.groupby('target_samples')['pearson_r'].agg(['mean', 'std']).reset_index()\n",
    "    grouped = grouped.sort_values('target_samples', ascending=False)\n",
    "    \n",
    "    x_labels = [f\"{t//1000}k\" if t >= 1000 else str(t) for t in grouped['target_samples']]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=x_labels,\n",
    "            y=grouped['mean'],\n",
    "            error_y=dict(type='data', array=grouped['std']),\n",
    "            name=method,\n",
    "            marker_color='blue' if method == 'interpolation' else 'green'\n",
    "        ),\n",
    "        row=1, col=col_idx\n",
    "    )\n",
    "    \n",
    "    # Add threshold line at 0.9\n",
    "    fig.add_hline(y=0.9, line_dash=\"dash\", line_color=\"red\", \n",
    "                  annotation_text=\"0.9 threshold\", row=1, col=col_idx)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Correlation with Baseline vs Downsampling Level\",\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_yaxes(title_text=\"Pearson r (mean across features)\", range=[0, 1])\n",
    "fig.update_xaxes(title_text=\"Target samples\")\n",
    "\n",
    "save_figure(fig, \"Feature_Correlation_vs_Downsampling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discriminative Power Analysis\n",
    "\n",
    "Test if features can still discriminate between drones at each downsampling level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_discriminative_power(df, target_samples, method):\n",
    "    \"\"\"Evaluate drone classification accuracy using temporal features.\"\"\"\n",
    "    subset = df[(df['target_samples'] == target_samples) & (df['method'] == method)]\n",
    "    \n",
    "    X = subset[feature_cols].values\n",
    "    y = subset['drone_code'].values\n",
    "    \n",
    "    # Handle NaN\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Random Forest with cross-validation\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    scores = cross_val_score(clf, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "\n",
    "# Evaluate at each level\n",
    "discriminative_results = []\n",
    "\n",
    "for target in DOWNSAMPLE_TARGETS:\n",
    "    for method in ['interpolation', 'decimation']:\n",
    "        acc_mean, acc_std = evaluate_discriminative_power(features_df, target, method)\n",
    "        factor, _ = get_downsample_factor(1_200_000, target)\n",
    "        \n",
    "        discriminative_results.append({\n",
    "            'target_samples': target,\n",
    "            'downsample_factor': factor,\n",
    "            'method': method,\n",
    "            'accuracy_mean': acc_mean,\n",
    "            'accuracy_std': acc_std\n",
    "        })\n",
    "        print(f\"{target:>10,} ({factor:>5.1f}x) {method:>15}: {acc_mean:.3f} +/- {acc_std:.3f}\")\n",
    "\n",
    "discrim_df = pd.DataFrame(discriminative_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot discriminative power\n",
    "fig = go.Figure()\n",
    "\n",
    "for method in ['interpolation', 'decimation']:\n",
    "    method_data = discrim_df[discrim_df['method'] == method].sort_values('target_samples', ascending=False)\n",
    "    x_labels = [f\"{t//1000}k\" if t >= 1000 else str(t) for t in method_data['target_samples']]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_labels,\n",
    "        y=method_data['accuracy_mean'],\n",
    "        error_y=dict(type='data', array=method_data['accuracy_std']),\n",
    "        mode='lines+markers',\n",
    "        name=method.capitalize(),\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Drone Classification Accuracy vs Downsampling Level (Random Forest on Temporal Features)\",\n",
    "    xaxis_title=\"Target samples\",\n",
    "    yaxis_title=\"5-fold CV Accuracy\",\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "save_figure(fig, \"Discriminative_Power_vs_Downsampling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spectral Analysis at Different Downsampling Levels\n",
    "\n",
    "Compare power spectral density to identify frequency content loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(iq_signal, fs, nperseg=1024):\n",
    "    \"\"\"Compute power spectral density.\"\"\"\n",
    "    freqs, psd = scipy_signal.welch(iq_signal, fs=fs, nperseg=min(nperseg, len(iq_signal)//4),\n",
    "                                     return_onesided=False)\n",
    "    # Sort by frequency\n",
    "    idx = np.argsort(freqs)\n",
    "    return freqs[idx], 10 * np.log10(psd[idx] + EPSILON_POWER)\n",
    "\n",
    "\n",
    "# Compare PSD at different levels\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for idx, target in enumerate(DOWNSAMPLE_TARGETS):\n",
    "    factor, eff_fs = get_downsample_factor(len(iq_baseline), target)\n",
    "    \n",
    "    # Use decimation (cleaner spectrum)\n",
    "    iq_down = downsampled_decim[target]\n",
    "    freqs, psd_db = compute_psd(iq_down, eff_fs)\n",
    "    \n",
    "    label = f\"{target//1000}k\" if target >= 1000 else str(target)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=freqs / 1e6,  # MHz\n",
    "        y=psd_db,\n",
    "        mode='lines',\n",
    "        name=f\"{label} ({factor:.0f}x)\",\n",
    "        line=dict(width=1, color=colors[idx % len(colors)])\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Power Spectral Density at Different Downsampling Levels (Decimation)\",\n",
    "    xaxis_title=\"Frequency (MHz)\",\n",
    "    yaxis_title=\"PSD (dB)\",\n",
    "    height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "save_figure(fig, \"PSD_Comparison_Downsampling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Burst Pattern Detection at Different Levels\n",
    "\n",
    "Verify if burst/packet structures remain visible after downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bursts(amplitude, threshold_percentile=75):\n",
    "    \"\"\"Simple burst detection based on amplitude threshold.\"\"\"\n",
    "    threshold = np.percentile(amplitude, threshold_percentile)\n",
    "    above_threshold = amplitude > threshold\n",
    "    \n",
    "    # Find burst boundaries (transitions)\n",
    "    transitions = np.diff(above_threshold.astype(int))\n",
    "    burst_starts = np.where(transitions == 1)[0] + 1\n",
    "    burst_ends = np.where(transitions == -1)[0] + 1\n",
    "    \n",
    "    return len(burst_starts), above_threshold\n",
    "\n",
    "\n",
    "# Compare burst detection across downsampling levels\n",
    "burst_results = []\n",
    "\n",
    "for target in DOWNSAMPLE_TARGETS:\n",
    "    for method_name, signals in [('interpolation', downsampled_interp), \n",
    "                                  ('decimation', downsampled_decim)]:\n",
    "        sig = signals[target]\n",
    "        n_bursts, _ = detect_bursts(np.abs(sig))\n",
    "        \n",
    "        burst_results.append({\n",
    "            'target_samples': target,\n",
    "            'method': method_name,\n",
    "            'n_bursts': n_bursts\n",
    "        })\n",
    "\n",
    "burst_df = pd.DataFrame(burst_results)\n",
    "print(\"Detected Bursts at Each Level:\")\n",
    "print(burst_df.pivot(index='target_samples', columns='method', values='n_bursts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of burst detection\n",
    "fig = make_subplots(\n",
    "    rows=len(DOWNSAMPLE_TARGETS), cols=1,\n",
    "    subplot_titles=[f\"{t//1000}k samples ({get_downsample_factor(1_200_000, t)[0]:.0f}x)\" \n",
    "                    if t >= 1000 else f\"{t} samples\" for t in DOWNSAMPLE_TARGETS],\n",
    "    vertical_spacing=0.03\n",
    ")\n",
    "\n",
    "# Use 5ms window for burst visibility\n",
    "for idx, target in enumerate(DOWNSAMPLE_TARGETS):\n",
    "    row = idx + 1\n",
    "    factor, eff_fs = get_downsample_factor(1_200_000, target)\n",
    "    \n",
    "    # 5ms window\n",
    "    viz_samples = int(eff_fs * 5 / 1000)\n",
    "    sig = downsampled_decim[target][:viz_samples]\n",
    "    amplitude = np.abs(sig)\n",
    "    \n",
    "    time_axis = np.arange(len(sig)) / eff_fs * 1e6  # us\n",
    "    \n",
    "    # Amplitude\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_axis, y=amplitude, mode='lines',\n",
    "                   line=dict(width=0.5, color='blue'), showlegend=False),\n",
    "        row=row, col=1\n",
    "    )\n",
    "    \n",
    "    # Threshold line\n",
    "    threshold = np.percentile(amplitude, 75)\n",
    "    fig.add_hline(y=threshold, line_dash=\"dash\", line_color=\"red\", \n",
    "                  line_width=0.5, row=row, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Burst Patterns at Different Downsampling Levels (5ms window, decimation)\",\n",
    "    height=200 * len(DOWNSAMPLE_TARGETS),\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Time (us)\", row=len(DOWNSAMPLE_TARGETS), col=1)\n",
    "\n",
    "save_figure(fig, \"Burst_Patterns_Downsampling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Drone Comparison at Optimal Downsampling\n",
    "\n",
    "Compare signals across different drones at the recommended downsampling level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal level (highest discriminative power with reasonable compression)\n",
    "optimal_df = discrim_df[discrim_df['method'] == 'decimation'].copy()\n",
    "optimal_df['efficiency'] = optimal_df['accuracy_mean'] * optimal_df['downsample_factor']\n",
    "\n",
    "# Select level with accuracy > 0.8 and highest compression\n",
    "good_levels = optimal_df[optimal_df['accuracy_mean'] > 0.7]\n",
    "if len(good_levels) > 0:\n",
    "    optimal_target = good_levels.loc[good_levels['downsample_factor'].idxmax(), 'target_samples']\n",
    "else:\n",
    "    optimal_target = 350_000  # Default conservative choice\n",
    "\n",
    "print(f\"Recommended downsampling target: {int(optimal_target):,} samples\")\n",
    "print(f\"Downsampling factor: {1_200_000 / optimal_target:.1f}x\")\n",
    "print(f\"Effective sampling rate: {config.FS / (1_200_000 / optimal_target) / 1e6:.2f} MHz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare signals across drones at optimal level\n",
    "fig = go.Figure()\n",
    "\n",
    "optimal_factor, optimal_fs = get_downsample_factor(1_200_000, optimal_target)\n",
    "viz_samples = int(optimal_fs * 2 / 1000)  # 2ms\n",
    "\n",
    "for drone in DRONES:\n",
    "    drone_file = df[(df['drone_code'] == drone) & \n",
    "                    (df['state'] == 'ON') & \n",
    "                    (df['interference'] == 'CLEAN')]\n",
    "    \n",
    "    if len(drone_file) == 0:\n",
    "        continue\n",
    "        \n",
    "    file_path = Path(drone_file.iloc[0]['file_path'])\n",
    "    iq_full = load_iq_segment(file_path, 0, SEGMENT_MS)\n",
    "    iq_down = downsample_decimate(iq_full, int(optimal_target))\n",
    "    \n",
    "    time_axis = np.arange(viz_samples) / optimal_fs * 1e6\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_axis,\n",
    "        y=np.abs(iq_down[:viz_samples]),\n",
    "        mode='lines',\n",
    "        name=drone,\n",
    "        line=dict(width=0.8)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Cross-Drone Comparison at {int(optimal_target)//1000}k samples ({optimal_factor:.1f}x downsampling)\",\n",
    "    xaxis_title=\"Time (us)\",\n",
    "    yaxis_title=\"Amplitude\",\n",
    "    height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "save_figure(fig, \"Cross_Drone_Comparison_Optimal\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = []\n",
    "\n",
    "for target in DOWNSAMPLE_TARGETS:\n",
    "    factor, eff_fs = get_downsample_factor(1_200_000, target)\n",
    "    \n",
    "    # Get correlation and accuracy for decimation method\n",
    "    corr_subset = corr_df[(corr_df['target_samples'] == target) & \n",
    "                          (corr_df['method'] == 'decimation')]\n",
    "    mean_corr = corr_subset['pearson_r'].mean() if len(corr_subset) > 0 else 1.0\n",
    "    \n",
    "    acc_subset = discrim_df[(discrim_df['target_samples'] == target) & \n",
    "                            (discrim_df['method'] == 'decimation')]\n",
    "    accuracy = acc_subset['accuracy_mean'].values[0] if len(acc_subset) > 0 else 0.0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Target Samples': f\"{target:,}\",\n",
    "        'Factor': f\"{factor:.1f}x\",\n",
    "        'Effective Fs (MHz)': f\"{eff_fs/1e6:.2f}\",\n",
    "        'Feature Correlation': f\"{mean_corr:.3f}\",\n",
    "        'Classification Acc.': f\"{accuracy:.3f}\",\n",
    "        'Memory Reduction': f\"{factor:.0f}x\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNSAMPLING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendations\n",
    "print(\"\"\"\n",
    "RECOMMENDATIONS FOR RF-UAVNet\n",
    "=============================\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "1. CURRENT APPROACH (10k samples, 120x downsampling):\n",
    "   - Loses significant temporal information\n",
    "   - Explains the poor 56% accuracy\n",
    "   - Effective Fs = 500 kHz (Nyquist = 250 kHz) - too low for drone RF patterns\n",
    "\n",
    "2. CONSERVATIVE RECOMMENDATION (350k samples, ~3.4x downsampling):\n",
    "   - Preserves most discriminative features (correlation > 0.95)\n",
    "   - Maintains burst/packet structure\n",
    "   - Effective Fs = 17.5 MHz (adequate for 2.4 GHz drone protocols)\n",
    "   - 3.4x memory reduction vs baseline\n",
    "\n",
    "3. MODERATE RECOMMENDATION (150k samples, 8x downsampling):\n",
    "   - Good balance of information preservation and compression\n",
    "   - May lose some fine-grained burst details\n",
    "   - 8x memory reduction\n",
    "\n",
    "4. AGGRESSIVE (40k samples, 30x downsampling):\n",
    "   - Significant information loss but still better than 10k\n",
    "   - Use only if memory is critical constraint\n",
    "\n",
    "METHOD RECOMMENDATION: Use scipy.signal.decimate with anti-aliasing filter\n",
    "instead of linear interpolation to prevent aliasing artifacts.\n",
    "\n",
    "NEXT STEPS:\n",
    "- Retrain RF-UAVNet with 350k or 150k input samples\n",
    "- Adapt model architecture (may need different conv kernel sizes)\n",
    "- Compare performance vs current 10k baseline\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
