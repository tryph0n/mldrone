{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DroneDetect V2 - PSD Feature Extraction\n",
    "\n",
    "## Overview\n",
    "This notebook implements PSD (Power Spectral Density) feature extraction for RF-based drone classification using classical machine learning approaches.\n",
    "\n",
    "## Methodology\n",
    "We follow a **two-stage pipeline**: (1) PSD Feature Generation → (2) SVM/Classical ML Training\n",
    "\n",
    "The PSD extraction process:\n",
    "1. Loads raw I/Q samples from .dat files (60 MHz sampling rate)\n",
    "2. **Normalizes per-file** (Z-score) before segmentation\n",
    "3. Segments continuous signals into fixed-duration windows (20ms)\n",
    "4. Computes **Power Spectral Density** via Welch method (nperseg=1024)\n",
    "5. **Per-sample normalization** (division by max, linear scale)\n",
    "6. Saves PSD features with encoded labels (drone type, interference, flight state)\n",
    "\n",
    "## Why PSD Features?\n",
    "\n",
    "**PSD**: Drone RF signatures exhibit distinct power distributions across frequencies due to communication protocols (WiFi, Bluetooth) and flight controller telemetry. PSD isolates these frequency-domain characteristics efficiently for classical ML classifiers.\n",
    "\n",
    "**Advantages**:\n",
    "- Compact representation (1024 features vs 150k for spectrograms)\n",
    "- Interpretable frequency peaks\n",
    "- Fast training with SVM/Random Forest\n",
    "- Works well with limited data\n",
    "\n",
    "## Parameter Selection (Aligned with RFClassification Reference)\n",
    "\n",
    "**Updated to match reference implementation:**\n",
    "- **FFT size (nperseg)**: **1024** - Frequency resolution: 60MHz/1024 ≈ 58.6 kHz/bin\n",
    "- **Segment duration**: **20ms** - Balance between computational cost and performance\n",
    "- **PSD conversion**: **Linear scale** (NO dB conversion) - Reference: REFERENTIEL Section 1.2.2\n",
    "- **Normalization**: **Per-file Z-score** before segmentation, then **per-sample** (psd/max)\n",
    "\n",
    "## Downstream Usage\n",
    "\n",
    "PSD features are consumed by:\n",
    "- `03_training_svm_COLAB.ipynb` - SVM classifier training\n",
    "- Classical ML models (Random Forest, XGBoost)\n",
    "\n",
    "## Reference Alignment\n",
    "All preprocessing parameters verified against REFERENTIEL_DRONEDETECT_RFCLASSIFICATION.md Section 1.2-1.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from dronedetect import config, data_loader, preprocessing, features\n",
    "\n",
    "# Memory monitoring utility\n",
    "\n",
    "def get_memory_mb():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "config.FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Initial memory: {get_memory_mb():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications\n",
    "\n",
    "All preprocessing parameters defined here for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPROCESSING SPECIFICATIONS\n",
    "# =============================================================================\n",
    "# All parameters controlling feature extraction defined here for reproducibility.\n",
    "\n",
    "# --- Signal Segmentation ---\n",
    "SEGMENT_DURATION_MS = 20        # Window length (config.DEFAULT_SEGMENT_MS)\n",
    "                                 # RFClassification results: 10ms→20ms→50ms improves 76.9%→83.6%→89.4%\n",
    "\n",
    "# --- Feature Extraction Parameters ---\n",
    "N_FFT = 1024                     # FFT size - Reference: REFERENTIEL Section 1.2.1 (nperseg=1024)\n",
    "                                 # Frequency resolution = 60MHz/1024 ≈ 58.6 kHz/bin\n",
    "\n",
    "SPECTROGRAM_SIZE = (224, 224)    # Output image dimensions (config.DEFAULT_SPEC_SIZE)\n",
    "                                 # Matches VGG16/ResNet input for transfer learning\n",
    "\n",
    "IQ_DOWNSAMPLE_TARGET = 1.2e6     # Downsampled IQ samples (config.DEFAULT_IQ_DOWNSAMPLE)\n",
    "                                 # Original: ~1.2M samples/segment → 10k for memory efficiency\n",
    "\n",
    "# --- Batch Processing ---\n",
    "BATCH_SIZE = 10                  # Files per batch (adjust based on available RAM)\n",
    "\n",
    "# --- Dataset Info ---\n",
    "SAMPLING_RATE_MHZ = 60           # DroneDetect V2 sampling frequency\n",
    "EXPECTED_SEGMENTS_PER_FILE = 100 # Approx (2s recording / 20ms window)\n",
    "\n",
    "print(\"Specifications loaded:\")\n",
    "print(f\"  Segment: {SEGMENT_DURATION_MS}ms | FFT: {N_FFT} | Batch: {BATCH_SIZE} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Architecture\n",
    "\n",
    "Modular pipeline system for flexible feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPROCESSING ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class PreprocessingStep:\n",
    "    \"\"\"Base class for preprocessing steps.\"\"\"\n",
    "    \n",
    "    def process(self, segment: np.ndarray) -> Any:\n",
    "        \"\"\"Process a signal segment.\n",
    "        \n",
    "        Args:\n",
    "            segment: Input signal segment\n",
    "            \n",
    "        Returns:\n",
    "            Processed output (type depends on step)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class PSDStep(PreprocessingStep):\n",
    "    \"\"\"Power Spectral Density via Welch method with per-sample normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, nfft=1024):\n",
    "        self.nfft = nfft\n",
    "    \n",
    "    def process(self, segment):\n",
    "        _, psd = features.compute_psd(segment, nfft=self.nfft)\n",
    "        # Per-sample normalization (reference: REFERENTIEL Section 1.3.1, line 220)\n",
    "        # Division by max with zero-protection\n",
    "        psd_max = np.max(psd)\n",
    "        if psd_max < 1e-15:  # Essentially zero power\n",
    "            return np.zeros_like(psd)\n",
    "        return psd / psd_max\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"PSDStep(nfft={self.nfft})\"\n",
    "\n",
    "\n",
    "class SpectrogramStep(PreprocessingStep):\n",
    "    \"\"\"Spectrogram via STFT + resize.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(224, 224)):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def process(self, segment):\n",
    "        return features.compute_spectrogram(segment, target_size=self.target_size)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SpectrogramStep(size={self.target_size})\"\n",
    "\n",
    "\n",
    "class DownsampleIQStep(PreprocessingStep):\n",
    "    \"\"\"Downsample IQ via linear interpolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_samples=10000):\n",
    "        self.target_samples = target_samples\n",
    "    \n",
    "    def process(self, segment):\n",
    "        return preprocessing.downsample_iq(segment, target_samples=self.target_samples)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DownsampleIQStep(n={self.target_samples})\"\n",
    "\n",
    "\n",
    "class FeaturePipeline:\n",
    "    \"\"\"Pipeline orchestrating multiple preprocessing steps.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, steps: List[PreprocessingStep]):\n",
    "        \"\"\"Initialize pipeline.\n",
    "        \n",
    "        Args:\n",
    "            name: Pipeline identifier (used for output filename)\n",
    "            steps: List of preprocessing steps to apply in order\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.steps = steps\n",
    "    \n",
    "    def process_segment(self, segment: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply all steps sequentially.\n",
    "        \n",
    "        Args:\n",
    "            segment: Input signal segment\n",
    "            \n",
    "        Returns:\n",
    "            Final processed features\n",
    "        \"\"\"\n",
    "        data = segment\n",
    "        for step in self.steps:\n",
    "            data = step.process(data)\n",
    "        return data\n",
    "    \n",
    "    def get_output_filename(self) -> str:\n",
    "        \"\"\"Get output filename for this pipeline.\"\"\"\n",
    "        return f\"{self.name}_features.npz\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        steps_str = ' → '.join([str(s) for s in self.steps])\n",
    "        return f\"Pipeline({self.name}): {steps_str}\"\n",
    "\n",
    "\n",
    "print(\"Preprocessing architecture loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Configuration\n",
    "\n",
    "Define preprocessing pipelines for different feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FeaturePipeline(\n",
    "    name='psd',\n",
    "    steps=[PSDStep(nfft=N_FFT)]\n",
    ")\n",
    "\n",
    "print(f\"Configured pipeline: {pipeline}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_loader.get_dataset_metadata(config.DATA_DIR)\n",
    "print(f\"Total files: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features (Batch Processing)\n",
    "\n",
    "Process files in batches to avoid memory saturation. Features are written progressively to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "drone_encoder = LabelEncoder()\n",
    "interference_encoder = LabelEncoder()\n",
    "state_encoder = LabelEncoder()\n",
    "\n",
    "drone_encoder.fit(df['drone_code'].unique())\n",
    "interference_encoder.fit(df['interference'].unique())\n",
    "state_encoder.fit(df['state'].unique())\n",
    "\n",
    "print(f\"Label encoders initialized\")\n",
    "print(f\"  Drones: {drone_encoder.classes_}\")\n",
    "print(f\"  Interference: {interference_encoder.classes_}\")\n",
    "print(f\"  States: {state_encoder.classes_}\")\n",
    "\n",
    "# Batch processing\n",
    "batch_files = []\n",
    "batch_idx = 0\n",
    "\n",
    "for batch_start in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_df = df.iloc[batch_start:batch_start + BATCH_SIZE]\n",
    "\n",
    "    # Storage for current batch\n",
    "    batch_features = []\n",
    "    labels_batch = []\n",
    "    file_ids_batch = []\n",
    "\n",
    "    print(f\"\\nBatch {batch_idx}: files {batch_start}-{batch_start + len(batch_df)}\")\n",
    "\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        try:\n",
    "            # Load IQ data\n",
    "            iq_data = data_loader.load_raw_iq(row['file_path'])\n",
    "\n",
    "            # CRITICAL: Z-score normalization for PSD/Spectrogram\n",
    "            iq_normalized = preprocessing.normalize(iq_data)\n",
    "\n",
    "            # Segment into 20ms windows\n",
    "            segments = preprocessing.segment_signal(iq_normalized, segment_ms=SEGMENT_DURATION_MS)\n",
    "\n",
    "            # Process each segment through pipeline\n",
    "            for seg in segments:\n",
    "                seg = seg.copy()  # Break view to allow memory release\n",
    "\n",
    "                # Extract features\n",
    "                feature_output = pipeline.process_segment(seg)\n",
    "                batch_features.append(feature_output)\n",
    "\n",
    "                # Encode labels\n",
    "                labels_batch.append({\n",
    "                    'drone': drone_encoder.transform([row['drone_code']])[0],\n",
    "                    'interference': interference_encoder.transform([row['interference']])[0],\n",
    "                    'state': state_encoder.transform([row['state']])[0]\n",
    "                })\n",
    "                file_ids_batch.append(idx)\n",
    "\n",
    "                del seg\n",
    "\n",
    "            # Free memory\n",
    "            del iq_data, iq_normalized, segments\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {row['file_path']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save batch to disk\n",
    "    batch_file = config.FEATURES_DIR / f'batch_{batch_idx:03d}.npz'\n",
    "    np.savez_compressed(\n",
    "        batch_file,\n",
    "        psd=np.array(batch_features),\n",
    "        labels=np.array(labels_batch, dtype=object),\n",
    "        file_ids=np.array(file_ids_batch, dtype=np.int32)\n",
    "    )\n",
    "    batch_files.append(batch_file)\n",
    "\n",
    "    print(f\"  Saved {len(batch_features)} samples to {batch_file.name}\")\n",
    "    print(f\"  Memory: {get_memory_mb():.0f} MB\")\n",
    "\n",
    "    # Clean batch data\n",
    "    del batch_features, labels_batch, file_ids_batch\n",
    "    gc.collect()\n",
    "\n",
    "    batch_idx += 1\n",
    "\n",
    "print(f\"\\nBatch processing complete: {len(batch_files)} batches saved\")\n",
    "print(f\"Memory: {get_memory_mb():.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Batches into Final Files\n",
    "\n",
    "Combine all batch files into single feature files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging batches (disk-backed memmap)...\")\n",
    "\n",
    "MERGE_CHUNK_SIZE = 5  # Process N batches at a time\n",
    "\n",
    "\n",
    "def merge_pipeline_chunked(pipeline_name, batch_files, chunk_size=5):\n",
    "    \"\"\"Merge batches for a single pipeline using disk-backed memory mapping.\n",
    "\n",
    "    This approach prevents memory saturation by:\n",
    "    1. Writing directly to disk via memmap (NO RAM accumulation)\n",
    "    2. Processing one pipeline at a time\n",
    "    3. Pre-allocating final array size\n",
    "    4. Keeping memmap for direct npz save (no RAM load)\n",
    "\n",
    "    Args:\n",
    "        pipeline_name: Name of pipeline ('psd', 'spectrogram', 'iq')\n",
    "        batch_files: List of batch file paths\n",
    "        chunk_size: Number of batches to process simultaneously\n",
    "\n",
    "    Returns:\n",
    "        dict with memmap + metadata (labels stay in RAM - small)\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{pipeline_name}] Processing {len(batch_files)} batches (disk-backed)...\")\n",
    "\n",
    "    # STEP 1: Calculate total size by scanning batches\n",
    "    print(f\"[{pipeline_name}] Scanning batches to determine size...\")\n",
    "    total_samples = 0\n",
    "    feature_shape = None\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        data = np.load(batch_file, allow_pickle=True)\n",
    "        batch_features = data[pipeline_name]\n",
    "        total_samples += len(batch_features)\n",
    "        if feature_shape is None:\n",
    "            feature_shape = batch_features.shape[1:]  # (1024,) or (224,224,3)\n",
    "        del data\n",
    "\n",
    "    print(f\"[{pipeline_name}] Total samples: {total_samples}, feature shape: {feature_shape}\")\n",
    "\n",
    "    # STEP 2: Create memmap file (writes directly to disk)\n",
    "    memmap_file = config.FEATURES_DIR / f'{pipeline_name}_features_X.mmap'\n",
    "    final_shape = (total_samples,) + feature_shape\n",
    "\n",
    "    print(f\"[{pipeline_name}] Creating memmap: {final_shape} ({np.prod(final_shape)*4/1e9:.2f} GB)\")\n",
    "    X_memmap = np.memmap(memmap_file, dtype='float32', mode='w+', shape=final_shape)\n",
    "\n",
    "    # Storage for labels (small, can stay in RAM)\n",
    "    all_drone_labels = []\n",
    "    all_interference_labels = []\n",
    "    all_state_labels = []\n",
    "    all_file_ids = []\n",
    "\n",
    "    # STEP 3: Fill memmap progressively\n",
    "    write_offset = 0\n",
    "\n",
    "    for chunk_start in tqdm(range(0, len(batch_files), chunk_size), desc=f\"{pipeline_name} chunks\"):\n",
    "        chunk_batch_files = batch_files[chunk_start:chunk_start + chunk_size]\n",
    "\n",
    "        for batch_file in chunk_batch_files:\n",
    "            data = np.load(batch_file, allow_pickle=True)\n",
    "\n",
    "            # Write features directly to disk\n",
    "            batch_features = data[pipeline_name]\n",
    "            n_samples = len(batch_features)\n",
    "            X_memmap[write_offset:write_offset + n_samples] = batch_features\n",
    "            write_offset += n_samples\n",
    "\n",
    "            # Collect labels (small)\n",
    "            labels = data['labels']\n",
    "            all_drone_labels.extend([label['drone'] for label in labels])\n",
    "            all_interference_labels.extend([label['interference'] for label in labels])\n",
    "            all_state_labels.extend([label['state'] for label in labels])\n",
    "            all_file_ids.append(data['file_ids'])\n",
    "\n",
    "            del data, batch_features\n",
    "\n",
    "        # Flush to disk periodically\n",
    "        X_memmap.flush()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"  Written {write_offset}/{total_samples} samples, RAM: {get_memory_mb():.0f} MB\")\n",
    "\n",
    "    # STEP 4: Final flush (keep memmap, do NOT load into RAM)\n",
    "    X_memmap.flush()\n",
    "    \n",
    "    final_file_ids = np.concatenate(all_file_ids, axis=0)\n",
    "\n",
    "    return {\n",
    "        'X_memmap': X_memmap,  # Return memmap directly\n",
    "        'memmap_file': memmap_file,\n",
    "        'memmap_shape': final_shape,\n",
    "        'y_drone': np.array(all_drone_labels, dtype=np.int32),\n",
    "        'y_interference': np.array(all_interference_labels, dtype=np.int32),\n",
    "        'y_state': np.array(all_state_labels, dtype=np.int32),\n",
    "        'file_ids': final_file_ids\n",
    "    }\n",
    "\n",
    "\n",
    "# Process each pipeline separately (reduces peak memory ~3x)\n",
    "final_data = {}\n",
    "# Process single pipeline\n",
    "print(f\"\\n{'='*60}\")\n",
    "final_data = merge_pipeline_chunked(\n",
    "    \"psd\",\n",
    "    batch_files,\n",
    "    chunk_size=MERGE_CHUNK_SIZE\n",
    ")\n",
    "print(f\"[psd] Shape: {final_data['memmap_shape']}\")\n",
    "print(f\"Memory after psd: {get_memory_mb():.0f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final shapes (memmap-backed):\")\n",
    "# Process single pipeline\n",
    "print(f\"  {pipeline.name}: {final_data['memmap_shape']}\")\n",
    "print(f\"  File IDs: {final_data['file_ids'].shape} (unique: {len(np.unique(final_data['file_ids']))})\")\n",
    "print(f\"Final memory: {get_memory_mb():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Final Features to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features for each pipeline (directly from memmap, no RAM load)\n",
    "print(\"Saving features to npz (streaming from memmap)...\")\n",
    "\n",
    "# Save single pipeline\n",
    "output_file = config.FEATURES_DIR / \"psd_features.npz\"\n",
    "    \n",
    "# Save directly from memmap (npz will read from disk as needed)\n",
    "np.savez_compressed(\n",
    "    output_file,\n",
    "    X=final_data['X_memmap'],  # Memmap array\n",
    "    y_drone=final_data['y_drone'],\n",
    "    y_interference=final_data['y_interference'],\n",
    "    y_state=final_data['y_state'],\n",
    "    file_ids=final_data['file_ids'],\n",
    "    drone_classes=drone_encoder.classes_,\n",
    "    interference_classes=interference_encoder.classes_,\n",
    "    state_classes=state_encoder.classes_,\n",
    "    # Metadata for frequency conversion (baseband to absolute RF)\n",
    "    fs=config.FS,\n",
    "    center_freq=config.CENTER_FREQ,\n",
    "    bandwidth=config.BANDWIDTH\n",
    ")\n",
    "print(f\"Saved psd features to {output_file}\")\n",
    "    \n",
    "# Clean up memmap\n",
    "del final_data['X_memmap']\n",
    "final_data['memmap_file'].unlink()\n",
    "\n",
    "# Free remaining memory\n",
    "del final_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nAll features saved successfully! Final RAM: {get_memory_mb():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify\n",
    "psd_data = np.load(config.FEATURES_DIR / 'psd_features.npz', allow_pickle=True)\n",
    "print(\"PSD features loaded:\")\n",
    "print(f\"  X shape: {psd_data['X'].shape}\")\n",
    "print(f\"  y_drone shape: {psd_data['y_drone'].shape}\")\n",
    "print(f\"  Drone classes: {psd_data['drone_classes']}\")\n",
    "print(f\"  Interference classes: {psd_data['interference_classes']}\")\n",
    "print(f\"  State classes: {psd_data['state_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Stratified Train/Test Split with File-Level Grouping\n",
    "\n",
    "### Problem: Temporal Autocorrelation and Data Leakage\n",
    "\n",
    "Each `.dat` file in DroneDetect V2 contains approximately 2 seconds of continuous RF signal sampled at 60 MHz. During preprocessing, this signal is segmented into ~100 overlapping windows of 20ms each.\n",
    "\n",
    "**Critical observation:** Consecutive segments from the same recording exhibit strong temporal autocorrelation. The RF characteristics (carrier frequency drift, hardware imperfections, environmental noise) remain largely constant within a single acquisition.\n",
    "\n",
    "If segments from the same source file appear in both training and test sets, the model may learn to recognize recording-specific artifacts rather than generalizable drone RF signatures. This constitutes **data leakage** and leads to overly optimistic performance estimates that fail to generalize to unseen recordings.\n",
    "\n",
    "### Solution: StratifiedGroupKFold\n",
    "\n",
    "We implement a file-grouped stratified split using `sklearn.model_selection.StratifiedGroupKFold`:\n",
    "\n",
    "1. **Grouping constraint**: All segments from a given `.dat` file are assigned to the same split (train OR test, never both)\n",
    "2. **Stratification**: Splits maintain the drone class distribution to ensure balanced representation\n",
    "3. **Validation**: An assertion verifies zero file overlap between splits\n",
    "\n",
    "This approach ensures that test set performance reflects the model's ability to generalize to entirely new recordings, providing a realistic estimate of real-world deployment accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "def get_stratified_file_split(X, y, file_ids, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data at FILE level to prevent data leakage.\n",
    "    \n",
    "    Segments from the same .dat file (~100 segments) will never appear\n",
    "    in both train and test sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Features (n_samples, ...)\n",
    "    y : array-like\n",
    "        Labels for stratification (n_samples,)\n",
    "    file_ids : array-like\n",
    "        Source file ID for each sample (n_samples,)\n",
    "    test_size : float\n",
    "        Approximate test set proportion (actual may vary due to file grouping)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_idx, test_idx : arrays\n",
    "        Indices for train/test split\n",
    "    \"\"\"\n",
    "    n_splits = int(1 / test_size)  # e.g., test_size=0.2 -> 5 splits -> 1 fold = 20%\n",
    "    \n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Take first fold as train/test split\n",
    "    train_idx, test_idx = next(sgkf.split(X, y, groups=file_ids))\n",
    "    \n",
    "    # Verify no file leakage\n",
    "    train_files = set(file_ids[train_idx])\n",
    "    test_files = set(file_ids[test_idx])\n",
    "    assert len(train_files & test_files) == 0, \"Data leakage detected: files in both splits\"\n",
    "    \n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "# Usage example (run after loading features):\n",
    "# psd_data = np.load(config.FEATURES_DIR / 'psd_features.npz')\n",
    "# X, y, file_ids = psd_data['X'], psd_data['y_drone'], psd_data['file_ids']\n",
    "# train_idx, test_idx = get_stratified_file_split(X, y, file_ids)\n",
    "# X_train, X_test = X[train_idx], X[test_idx]\n",
    "# y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print(\"Split function defined: get_stratified_file_split(X, y, file_ids, test_size=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B: Data Augmentation Strategy for MA1/MAV\n",
    "\n",
    "**Note**: Data augmentation is applied during **model training**, not preprocessing. This section documents the recommended strategy for implementation in training notebooks.\n",
    "\n",
    "**Problem**: Class confusion between MA1/MAV requires targeted augmentation to improve discriminative feature learning.\n",
    "\n",
    "**Recommended Approaches**:\n",
    "\n",
    "1. **Differential Class Weighting** (Training phase)\n",
    "   - Increase loss weight for MA1/MAV samples (2x) to force model attention on subtle differences\n",
    "   - Implementation: `class_weight` parameter in sklearn, `sample_weight` in training loop\n",
    "   \n",
    "2. **SMOTE/ADASYN** (Training phase)\n",
    "   - Synthetic oversampling of minority class if MA1/MAV have fewer samples\n",
    "   - Use `imbalanced-learn` library: `from imblearn.over_sampling import SMOTE`\n",
    "   \n",
    "3. **Signal-Specific Augmentation** (Training phase)\n",
    "   - Time shifting: Roll segments by random offset\n",
    "   - Frequency masking: Zero out random frequency bands (SpecAugment-style)\n",
    "   - Noise injection: Add Gaussian noise to simulate varying SNR conditions\n",
    "\n",
    "**References**:\n",
    "- Chawla et al. (2002), \"SMOTE: Synthetic Minority Over-sampling Technique\", JAIR\n",
    "- Park et al. (2019), \"SpecAugment: A Simple Data Augmentation Method for ASR\", Interspeech\n",
    "- Huang et al. (2023), \"SigAugment: Automatic data augmentation for modulated radio signals\", Applied Sciences\n",
    "\n",
    "**Implementation Note**: \n",
    "Apply augmentation weights inversely proportional to classification accuracy:\n",
    "```python\n",
    "# Example for training notebook\n",
    "augmentation_weights = {\n",
    "    'AIR': 1.0, 'DIS': 1.0, 'INS': 1.0, 'MIN': 1.0, 'PHA': 1.0,\n",
    "    'MA1': 2.0,  # Higher weight due to confusion\n",
    "    'MAV': 2.0   # Higher weight due to confusion\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
