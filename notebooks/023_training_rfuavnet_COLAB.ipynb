{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Le3bq6gGQtw"
   },
   "source": [
    "# DroneDetect V2 - RF-UAVNet Training\n",
    "\n",
    "**Reference Paper:** Huynh-The et al. (2022), \"RF-UAVNet: High-Performance Convolutional Network for RF-Based Drone Surveillance Systems\", *IEEE Access*, Vol. 10, pp. 49143-49154.  \n",
    "DOI: [10.1109/ACCESS.2022.3173181](https://doi.org/10.1109/ACCESS.2022.3173181)\n",
    "\n",
    "## Overview\n",
    "\n",
    "RF-UAVNet is a lightweight 1D CNN architecture designed for RF-based drone surveillance. This implementation adapts the IEEE paper's approach to the DroneDetect V2 dataset.\n",
    "\n",
    "### Architecture Summary\n",
    "- **Input**: Raw IQ signals (2 channels: real + imaginary, 10,000 samples)\n",
    "- **R-Unit**: Initial feature extraction (Conv1d 2→64, k=5, s=5)\n",
    "- **G-Units**: 4x grouped convolutions (64→64, k=3, s=2, groups=8) with skip connections\n",
    "- **Multi-GAP**: Multi-scale global average pooling (kernels: 1000, 500, 250, 125)\n",
    "- **Classifier**: Fully connected layer (320 → num_classes)\n",
    "- **Total parameters**: 9,991 (~1800x smaller than VGG16)\n",
    "\n",
    "### Dataset Comparison: DroneRF (paper) vs DroneDetect V2 (ours)\n",
    "\n",
    "| Aspect | DroneRF (paper) | DroneDetect V2 (ours) | Impact |\n",
    "|--------|-----------------|----------------------|--------|\n",
    "| **Samples** | ~50,000 | 19,478 | **2.5x fewer samples** → higher overfitting risk |\n",
    "| **Drones** | 10 classes | 7 classes | Simpler task |\n",
    "| **Interference** | Unknown | 4 conditions (CLEAN/WIFI/BLUE/BOTH) | More realistic variability |\n",
    "| **Split method** | Segment-level (with leakage) | File-level stratified | Scientifically valid but harder |\n",
    "\n",
    "**Key trade-off:** Our model has **82% fewer parameters** (9,991 vs 53M for VGG16), making it ideal for edge deployment, but our **smaller dataset** (2.5x less data) limits generalization. The paper's reported 98.53% accuracy includes **data leakage** (see RFClassification analysis); our file-level split ensures valid generalization estimates.\n",
    "\n",
    "### Hyperparameters: Aligned with IEEE Paper\n",
    "\n",
    "Training configuration follows Huynh-The et al. (2022):\n",
    "- **Optimizer**: SGD with momentum (0.95) and weight decay (1e-4)\n",
    "- **Learning rate**: 0.01 (10x higher than typical Adam)\n",
    "- **Batch size**: 512 (paper uses 512)\n",
    "- **Epochs**: 50 (paper uses 120, reduced for computational cost)\n",
    "- **Scheduler**: ReduceLROnPlateau (factor=0.5, patience=5)\n",
    "\n",
    "These hyperparameters differ significantly from image CNNs (Adam, lr=0.001) due to RF signal characteristics.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "**1. Transfer Learning** (highest priority)\n",
    "- Pre-train on DroneRF dataset (50k samples) if accessible\n",
    "- Fine-tune final layers on DroneDetect V2\n",
    "- Expected gain: +10-20% accuracy (mitigates small dataset issue)\n",
    "\n",
    "**2. Data Augmentation**\n",
    "- Time shifting (circular roll): `np.roll(iq, shift, axis=-1)`\n",
    "- Noise injection (AWGN): `iq + noise`\n",
    "- Expected gain: +5-10% accuracy\n",
    "\n",
    "**3. Ensemble Methods**\n",
    "- Combine RF-UAVNet + SVM (PSD) + VGG16 (spectrograms)\n",
    "- Majority voting or stacking\n",
    "- Expected gain: +3-5% accuracy\n",
    "\n",
    "**4. Segment Duration Optimization**\n",
    "- Test 50ms segments (paper: 10ms→20ms→50ms: 76.9%→83.6%→89.4%)\n",
    "- Modify `config.DEFAULT_SEGMENT_MS` from 20 to 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chZMt1dyGQtz"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZinIAxKGQt0",
    "outputId": "cd840350-eb46-4325-8320-21e1e444b941"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtyqEqClGQt1",
    "outputId": "d0ab3615-dc0d-4460-e470-60491f9b4fdf"
   },
   "outputs": [],
   "source": [
    "!ls drive/MyDrive/DroneDetect_V2/output/features/iq_features.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baJgD6oSGQt2"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucTxATcUm9_d",
    "outputId": "122e98c7-6054-4ce2-b637-b85d59a1ce5c"
   },
   "outputs": [],
   "source": [
    "!pip install -U kaleido==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuJTA2LuGQt2",
    "outputId": "4d402fc6-1bae-4587-d341-e77652f9fbc7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setup figure saving\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_NAME = \"training_rfuavnet_COLAB\"\n",
    "FIGURES_DIR = Path(\"figures\") / NOTEBOOK_NAME\n",
    "\n",
    "def save_figure(fig) -> None:\n",
    "    \"\"\"Save plotly figure to PNG file using the figure'''s title as filename.\"\"\"\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    title = fig.layout.title.text if fig.layout.title.text else \"untitled\"\n",
    "    filename = re.sub(r'''[^\\w\\s-]''', '''''', title).strip()\n",
    "    filename = re.sub(r'''[\\s-]+''', '''_''', filename)\n",
    "    filepath = FIGURES_DIR / f\"{filename}.png\"\n",
    "    try:\n",
    "        fig.write_image(str(filepath), width=1200, height=800)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save figure (kaleido required): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvB1lVTmGQt2"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VZwkgbzGQt3",
    "outputId": "1d428d20-82ce-4a72-c0a7-19cbe32f7904"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    'features_path': 'drive/MyDrive/DroneDetect_V2/output/features/iq_features.npz',\n",
    "    'models_dir': 'drive/MyDrive/DroneDetect_V2/output/models/',\n",
    "    'test_data_dir': 'drive/MyDrive/DroneDetect_V2/output/sample/test_data/',\n",
    "\n",
    "    # Split parameters\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "\n",
    "    # Training parameters (aligned with Huynh-The et al. 2022, IEEE Access)\n",
    "    'batch_size': 512,          # Paper: 512 (paper specification)\n",
    "    'epochs': 120,               # Paper: 120 (paper specification)\n",
    "    'learning_rate': 0.01,      # Paper: 0.01 (SGD with momentum)\n",
    "    'momentum': 0.95,           # SGD momentum (paper specification)\n",
    "    'weight_decay': 1e-4,       # L2 regularization (paper specification)\n",
    "    'scheduler_factor': 0.5,    # LR reduction factor\n",
    "    'scheduler_patience': 5,    # Epochs before LR reduction\n",
    "\n",
    "    # Device\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(f\"Configuration (aligned with RF-UAVNet IEEE paper): {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc9thk57GQt3"
   },
   "source": [
    "## RF-UAV-Net Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuPeDj2vGQt4",
    "outputId": "75d4b1c4-05b2-452d-968b-cded13805767"
   },
   "outputs": [],
   "source": [
    "class RFUAVNet(nn.Module):\n",
    "    \"\"\"RF-UAVNet: 1D CNN architecture for RF-based drone classification.\n",
    "\n",
    "    This model is based on the architecture proposed by Huynh-The et al. (2022) in\n",
    "    \"RF-UAVNet: High-Performance Convolutional Network for RF-Based Drone Surveillance Systems\".\n",
    "    It processes raw IQ signals (Real/Imaginary) through a series of specialized units\n",
    "    (R-Unit, G-Units) and multi-scale pooling to classify drone signals.\n",
    "\n",
    "    Attributes:\n",
    "        conv_r (nn.Conv1d): Initial convolution layer (R-Unit).\n",
    "        bn_r (nn.BatchNorm1d): Batch normalization for R-Unit.\n",
    "        elu_r (nn.ELU): ELU activation function.\n",
    "        g_convs (nn.ModuleList): List of 4 grouped convolutional layers (G-Units).\n",
    "        g_bns (nn.ModuleList): List of batch normalization layers for G-Units.\n",
    "        g_elus (nn.ModuleList): List of ELU activations for G-Units.\n",
    "        pool (nn.MaxPool1d): Max pooling layer used in skip connections.\n",
    "        gap1000 (nn.AvgPool1d): Global Average Pooling with kernel size 1000.\n",
    "        gap500 (nn.AvgPool1d): Global Average Pooling with kernel size 500.\n",
    "        gap250 (nn.AvgPool1d): Global Average Pooling with kernel size 250.\n",
    "        gap125 (nn.AvgPool1d): Global Average Pooling with kernel size 125.\n",
    "        fc (nn.Linear): Fully connected output layer.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of output classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # R-unit\n",
    "        self.conv_r = nn.Conv1d(2, 64, kernel_size=5, stride=5)\n",
    "        self.bn_r = nn.BatchNorm1d(64)\n",
    "        self.elu_r = nn.ELU()\n",
    "\n",
    "        # G-units (4x)\n",
    "        self.g_convs = nn.ModuleList([\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, groups=8)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        self.g_bns = nn.ModuleList([nn.BatchNorm1d(64) for _ in range(4)])\n",
    "        self.g_elus = nn.ModuleList([nn.ELU() for _ in range(4)])\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Multi-scale GAP\n",
    "        self.gap1000 = nn.AvgPool1d(1000)\n",
    "        self.gap500 = nn.AvgPool1d(500)\n",
    "        self.gap250 = nn.AvgPool1d(250)\n",
    "        self.gap125 = nn.AvgPool1d(125)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(320, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 2, 10000).\n",
    "                Channels are [Real, Imaginary].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Raw logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # R-unit\n",
    "        x = self.elu_r(self.bn_r(self.conv_r(x)))\n",
    "\n",
    "        # G-units with residual connections\n",
    "        g_outputs = []\n",
    "        for i in range(4):\n",
    "            g_out = self.g_elus[i](self.g_bns[i](self.g_convs[i](F.pad(x, (1, 0)))))\n",
    "            g_outputs.append(g_out)\n",
    "            x = g_out + self.pool(x)\n",
    "\n",
    "        # Multi-scale GAP\n",
    "        gaps = [\n",
    "            self.gap1000(g_outputs[0]),\n",
    "            self.gap500(g_outputs[1]),\n",
    "            self.gap250(g_outputs[2]),\n",
    "            self.gap125(g_outputs[3]),\n",
    "            self.gap125(x)\n",
    "        ]\n",
    "\n",
    "        x = torch.cat(gaps, dim=1).flatten(start_dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def reset_weights(self):\n",
    "        \"\"\"Resets the model weights using the default initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "\n",
    "print(\"RF-UAV-Net model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6t0f6WbGQt4"
   },
   "source": [
    "## File-Level Stratified Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkQIE2JqGQt4",
    "outputId": "4bd28f87-eb97-4417-cdd3-8090b22f9a4d"
   },
   "outputs": [],
   "source": [
    "def get_stratified_file_split(X, y, file_ids, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data at FILE level to prevent data leakage.\n",
    "\n",
    "    Segments from the same .dat file (~100 segments) will never appear\n",
    "    in both train and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Features (n_samples, ...)\n",
    "    y : array-like\n",
    "        Labels for stratification (n_samples,)\n",
    "    file_ids : array-like\n",
    "        Source file ID for each sample (n_samples,)\n",
    "    test_size : float\n",
    "        Approximate test set proportion (actual may vary due to file grouping)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_idx, test_idx : arrays\n",
    "        Indices for train/test split\n",
    "    \"\"\"\n",
    "    n_splits = int(1 / test_size)  # e.g., test_size=0.2 -> 5 splits -> 1 fold = 20%\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Take first fold as train/test split\n",
    "    train_idx, test_idx = next(sgkf.split(X, y, groups=file_ids))\n",
    "\n",
    "    # Verify no file leakage\n",
    "    train_files = set(file_ids[train_idx])\n",
    "    test_files = set(file_ids[test_idx])\n",
    "    assert len(train_files & test_files) == 0, \"Data leakage detected: files in both splits\"\n",
    "\n",
    "    return train_idx, test_idx\n",
    "\n",
    "print(\"Stratified file split function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G69MRxtKGQt5"
   },
   "source": [
    "## 1. Load IQ Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-C_QAHmGQt5",
    "outputId": "a6f98886-20bc-4076-9d4e-fc25345b7fe3"
   },
   "outputs": [],
   "source": [
    "# Pattern 1: Memory mapping to avoid loading full array into RAM\n",
    "data = np.load(CONFIG['features_path'], mmap_mode='r',\n",
    "               allow_pickle=True)\n",
    "\n",
    "X = data['X']  # Shape: (N, 2, 10000) - memory mapped, not loaded\n",
    "y_drone = data['y_drone']\n",
    "y_interference = data['y_interference']\n",
    "y_state = data['y_state']\n",
    "file_ids = data['file_ids']  # For stratified file-level splitting\n",
    "\n",
    "drone_classes = data['drone_classes']\n",
    "interference_classes = data['interference_classes']\n",
    "state_classes = data['state_classes']\n",
    "\n",
    "print(f\"IQ data shape: {X.shape}\")\n",
    "print(f\"Drone labels shape: {y_drone.shape}\")\n",
    "print(f\"File IDs shape: {file_ids.shape} (unique files: {len(np.unique(file_ids))})\")\n",
    "print(f\"Drone classes: {drone_classes}\")\n",
    "print(f\"Interference classes: {interference_classes}\")\n",
    "print(f\"State classes: {state_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SuFzh4FGQt5"
   },
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "We'll use 80/20 split with file-level stratification to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWd_hWxJGQt5",
    "outputId": "4ed85773-c19b-4dac-864c-09ed5d9f13e0"
   },
   "outputs": [],
   "source": [
    "# Split for drone classification using file-level stratification\n",
    "train_idx, test_idx = get_stratified_file_split(\n",
    "    X, y_drone, file_ids,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "# Pattern 2: Zero-copy split (use views, not copies)\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y_drone[train_idx], y_drone[test_idx]\n",
    "y_interference_test = y_interference[test_idx]\n",
    "y_state_test = y_state[test_idx]\n",
    "\n",
    "# Verify no file leakage\n",
    "train_files = set(file_ids[train_idx])\n",
    "test_files = set(file_ids[test_idx])\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")\n",
    "print(f\"File overlap: {len(train_files & test_files)} (should be 0)\")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Save test data for reuse\n",
    "os.makedirs(CONFIG['test_data_dir'], exist_ok=True)\n",
    "\n",
    "# Save full test data with interference and state metadata\n",
    "test_data_path = os.path.join(CONFIG['test_data_dir'], 'rfuavnet_test_data.npz')\n",
    "np.savez(\n",
    "    test_data_path,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    y_interference_test=y_interference_test,\n",
    "    y_state_test=y_state_test,\n",
    "    test_idx=test_idx,\n",
    "    file_ids_test=file_ids[test_idx],\n",
    "    drone_classes=drone_classes,\n",
    "    interference_classes=interference_classes,\n",
    "    state_classes=state_classes\n",
    ")\n",
    "print(f\"\\nFull test data saved to {test_data_path}\")\n",
    "\n",
    "# Save separated files per Drone and Interference (Hierarchical)\n",
    "print(\"\\nGenerating separated test files (structure: iq/INT/DRONE/)...\")\n",
    "\n",
    "for d_idx, drone_class in enumerate(drone_classes):\n",
    "    for i_idx, int_class in enumerate(interference_classes):\n",
    "        # Filter for specific drone and interference\n",
    "        mask = (y_test == d_idx) & (y_interference_test == i_idx)\n",
    "\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        X_sub = X_test[mask]\n",
    "        y_sub = y_test[mask]\n",
    "        y_int_sub = y_interference_test[mask]\n",
    "\n",
    "        # Define components for hierarchy and filename\n",
    "        data_type = 'iq'\n",
    "        int_name = str(int_class)\n",
    "        drone_name = str(drone_class)\n",
    "        duration = '20' # 20ms fixed duration\n",
    "\n",
    "        # Create directory structure: output/sample/test_data/{INT}/\n",
    "        save_dir = os.path.join(CONFIG['test_data_dir'], int_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Construct filename: iq_{INT}_{DRONE}_20.npz\n",
    "        filename = f\"{data_type}_{int_name}_{drone_name}_{duration}.npz\"\n",
    "        file_path = os.path.join(save_dir, filename)\n",
    "\n",
    "        np.savez(\n",
    "            file_path,\n",
    "            X=X_sub,\n",
    "            y=y_sub,\n",
    "            y_interference=y_int_sub,\n",
    "            drone_class=drone_class,\n",
    "            interference_class=int_class\n",
    "        )\n",
    "        print(f\"  Saved {filename} in {save_dir} ({len(X_sub)} samples)\")\n",
    "\n",
    "# Cleanup: delete references to full array and indices\n",
    "del X, data, train_idx, test_idx\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleanup: X, data, indices deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTYIl6rQGQt5"
   },
   "source": [
    "## 3. Prepare PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cc0erLv9GQt5",
    "outputId": "2c954975-e04e-4d86-c15f-d9496f774041"
   },
   "outputs": [],
   "source": [
    "# Pattern 3: PyTorch conversion with immediate cleanup\n",
    "# Convert train set\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).long()\n",
    "del X_train, y_train  # Delete numpy arrays immediately\n",
    "gc.collect()\n",
    "\n",
    "# Convert test set\n",
    "X_test_t = torch.from_numpy(X_test).float()\n",
    "y_test_t = torch.from_numpy(y_test).long()\n",
    "del X_test, y_test  # Delete numpy arrays immediately\n",
    "gc.collect()\n",
    "\n",
    "print(\"PyTorch tensors created and NumPy arrays deleted\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHjlgL2GGQt5"
   },
   "source": [
    "## 4. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qyTM_iBGQt5",
    "outputId": "a2a03b5d-a601-4a89-9763-786347ead2f7"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, config):\n",
    "    \"\"\"\n",
    "    Train RF-UAVNet with IEEE paper hyperparameters.\n",
    "\n",
    "    Optimizer: SGD with momentum (0.95) and weight decay (1e-4)\n",
    "    Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\n",
    "    \"\"\"\n",
    "    model = model.to(config['device'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # SGD optimizer (as per IEEE paper)\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        momentum=config['momentum'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=config['scheduler_factor'],\n",
    "        patience=config['scheduler_patience'],\n",
    "    )\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} - Train\"):\n",
    "            batch_x, batch_y = batch_x.to(config['device']), batch_y.to(config['device'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += batch_y.size(0)\n",
    "            train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(config['device']), batch_y.to(config['device'])\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, LR={current_lr:.6f}\")\n",
    "\n",
    "        # Periodic cleanup\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"  Memory cleanup at epoch {epoch+1}\")\n",
    "\n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function defined (SGD + ReduceLROnPlateau)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcyjXabfGQt6"
   },
   "source": [
    "## 5. Train RF-UAV-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qc61ZIPw0kEj",
    "outputId": "91a78e56-4a35-4076-bc14-a08649494969"
   },
   "outputs": [],
   "source": [
    "num_classes = len(drone_classes)\n",
    "rfuavnet = RFUAVNet(num_classes=num_classes)\n",
    "print(rfuavnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijdP_Jx9GQt6",
    "outputId": "18c4fcd2-5314-4784-dee1-6c63108c811c"
   },
   "outputs": [],
   "source": [
    "print(f\"Training RF-UAVNet with {num_classes} classes...\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in rfuavnet.parameters()):,}\")\n",
    "print(f\"Paper comparison: VGG16 has ~138M parameters (1800x larger)\\n\")\n",
    "\n",
    "rfuavnet, history = train_model(rfuavnet, train_loader, test_loader, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOl1jCcDGQt6"
   },
   "source": [
    "## 6. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "ut0Lr5ysGQt6",
    "outputId": "e0875139-8842-48ee-9e0a-8ec40bf647a1"
   },
   "outputs": [],
   "source": [
    "# Training history visualization with plotly\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('RF-UAVNet Loss', 'RF-UAVNet Accuracy'))\n",
    "\n",
    "epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "\n",
    "# Loss subplot\n",
    "fig.add_trace(go.Scatter(x=epochs, y=history['train_loss'], mode='lines+markers', name='Train Loss', line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=history['val_loss'], mode='lines+markers', name='Val Loss', line=dict(color='orange')), row=1, col=1)\n",
    "\n",
    "# Accuracy subplot\n",
    "fig.add_trace(go.Scatter(x=epochs, y=history['train_acc'], mode='lines+markers', name='Train Acc', line=dict(color='blue')), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=epochs, y=history['val_acc'], mode='lines+markers', name='Val Acc', line=dict(color='orange')), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='RF-UAVNet Training History',\n",
    "    height=500,\n",
    "    width=1200\n",
    ")\n",
    "fig.update_xaxes(title_text='Epoch', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Epoch', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twua1s-dGQt6"
   },
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kXYYxEtGQt6",
    "outputId": "da7086c2-1a91-41bf-b869-533ee315c0bb"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set with efficient memory usage.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(CONFIG['device'])\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Pattern 6: Append to lists directly (avoid unnecessary copies)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "preds, labels = evaluate_model(rfuavnet, test_loader)\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "print(f\"RF-UAV-Net Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"RF-UAV-Net Test F1-Score (weighted): {f1:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=drone_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n32fV5AGQt6"
   },
   "source": [
    "## 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "Xlq2nKiiGQt6",
    "outputId": "99602d81-b7d3-4fba-a741-035ddefe16c3"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Create confusion matrix heatmap with plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=list(drone_classes),\n",
    "    y=list(drone_classes),\n",
    "    colorscale='Purples',\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={'size': 12},\n",
    "    hoverongaps=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'RF-UAVNet Confusion Matrix - Accuracy: {accuracy:.4f}',\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='True',\n",
    "    xaxis={'side': 'bottom'},\n",
    "    yaxis={'autorange': 'reversed'},\n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMdqiei6GQt7"
   },
   "source": [
    "## 9. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MLrLQMNMGQt7",
    "outputId": "449b5972-03b5-47bb-8a3c-812833269bb6"
   },
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "    labels, preds, labels=range(len(drone_classes)), zero_division=0\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for display\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': drone_classes,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1_per_class,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Precision plot\n",
    "fig_precision = px.bar(metrics_df, x='Class', y='Precision', title='RF-UAVNet Precision per Class',\n",
    "                       color='Precision', range_y=[0, 1.05])\n",
    "fig_precision.update_layout(xaxis_title=\"Class\", yaxis_title=\"Precision\", height=400)\n",
    "fig_precision.show()\n",
    "save_figure(fig_precision)\n",
    "\n",
    "# Recall plot\n",
    "fig_recall = px.bar(metrics_df, x='Class', y='Recall', title='RF-UAVNet Recall per Class',\n",
    "                    color='Recall', color_continuous_scale=px.colors.sequential.Oranges, range_y=[0, 1.05])\n",
    "fig_recall.update_layout(xaxis_title=\"Class\", yaxis_title=\"Recall\", height=400)\n",
    "fig_recall.show()\n",
    "save_figure(fig_recall)\n",
    "\n",
    "# F1-Score plot\n",
    "fig_f1 = px.bar(metrics_df, x='Class', y='F1-Score', title='RF-UAVNet F1-Score per Class',\n",
    "                color='F1-Score', color_continuous_scale=px.colors.sequential.Greens, range_y=[0, 1.05])\n",
    "fig_f1.update_layout(xaxis_title=\"Class\", yaxis_title=\"F1-Score\", height=400)\n",
    "fig_f1.show()\n",
    "save_figure(fig_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP_qf0I4GQt7"
   },
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVUfC5PaGQt7",
    "outputId": "8e8c0222-61e4-48b9-fac8-c0825d5f0bc0"
   },
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "os.makedirs(CONFIG['models_dir'], exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(CONFIG['models_dir'], 'rfuavnet_iq.pth')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': rfuavnet.state_dict(),\n",
    "    'classes': drone_classes,\n",
    "    'accuracy': accuracy,\n",
    "    'f1': f1,\n",
    "    'history': history,\n",
    "    'config': CONFIG\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1CkUBemGQt8"
   },
   "source": [
    "## 12. Summary\n",
    "\n",
    "Key takeaways:\n",
    "- RF-UAV-Net processes raw IQ data (2 channels, 10000 samples)\n",
    "- Architecture: R-Unit (Conv1d 2->64) + 4 G-Units (grouped convolutions)\n",
    "- Multi-scale GAP for feature aggregation\n",
    "- File-level stratified split prevents data leakage\n",
    "- Lower learning rate (0.001) compared to CNN (0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCb1b1UWGQt8",
    "outputId": "530e601a-a035-450a-fbb7-e2cead43fb2a"
   },
   "outputs": [],
   "source": [
    "print(\"=== RF-UAV-Net Training Summary ===\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  Total samples: {len(X_train_t) + len(X_test_t)}\")\n",
    "print(f\"  Training samples: {len(X_train_t)}\")\n",
    "print(f\"  Test samples: {len(X_test_t)}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "\n",
    "print(f\"Performance:\")\n",
    "print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Test F1-Score: {f1:.4f}\")\n",
    "print(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
