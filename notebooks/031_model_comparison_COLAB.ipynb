{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljXr44U5DWKA"
   },
   "source": [
    "# DroneDetect V2 - Model Comparison (V3)\n",
    "\n",
    "Comprehensive comparison of all trained models:\n",
    "- **SVM** (PSD features)\n",
    "- **VGG16** (Spectrogram features - RGB via Viridis colormap)\n",
    "- **ResNet50** (Spectrogram features - RGB via Viridis colormap)\n",
    "- **RF-UAV-Net** (Raw IQ features)\n",
    "\n",
    "Evaluation metrics:\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion matrices\n",
    "- Statistical significance tests (McNemar, Bootstrap CI, Cohen's kappa)\n",
    "- **Per-sample inference time with proper benchmarking**\n",
    "- Model size comparison\n",
    "- Error analysis\n",
    "- **Sample size comparison**\n",
    "- **Test sample export to GDrive**\n",
    "\n",
    "**Inference timing methodology:**\n",
    "- Per-sample timing with warm-up (10 iterations)\n",
    "- 100 timing runs per model\n",
    "- Reports p50, p95, p99 latencies (median recommended for comparison)\n",
    "- GPU synchronization for accurate CUDA timing\n",
    "- Uses `time.perf_counter()` for high-resolution measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DY2NkXzDWKB"
   },
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s6vBgyUDWKB",
    "outputId": "d62a0b62-2aa6-4a15-ccd4-7751cd74ca67"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuEEd4VHDWKB"
   },
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SxIkm8zDWKC",
    "outputId": "b6189bfa-3f5a-44d6-8b9b-749ac091f3c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as tv_models\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    f1_score, precision_recall_fscore_support, cohen_kappa_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import chi2\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Figure saving configuration\n",
    "NOTEBOOK_NAME = \"model_comparison_COLAB_V3\"\n",
    "FIGURES_DIR = Path(\"figures\") / NOTEBOOK_NAME\n",
    "\n",
    "\n",
    "def save_figure(fig) -> None:\n",
    "    \"\"\"Save plotly figure to PNG file using the figure's title as filename.\"\"\"\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    title = fig.layout.title.text if fig.layout.title.text else \"untitled\"\n",
    "    filename = re.sub(r'[^\\w\\s-]', '', title).strip()\n",
    "    filename = re.sub(r'[\\s-]+', '_', filename)\n",
    "    filepath = FIGURES_DIR / f\"{filename}.png\"\n",
    "    try:\n",
    "        fig.write_image(str(filepath), width=1200, height=800)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save figure (kaleido required): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-u8PFLTDWKC"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AP_nGUklDWKC",
    "outputId": "1172f37c-8192-4a8c-eb50-734158707524"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    'psd_path': 'drive/MyDrive/DroneDetect_V2/output/features/psd_features.npz',\n",
    "    'spectrogram_path': 'drive/MyDrive/DroneDetect_V2/output/features/spectrogram_features.npz',\n",
    "    'iq_path': 'drive/MyDrive/DroneDetect_V2/output/features/iq_features.npz',\n",
    "    'models_dir': 'drive/MyDrive/DroneDetect_V2/output/models/',\n",
    "\n",
    "    # Sample export path\n",
    "    'sample_export_dir': 'drive/MyDrive/DroneDetect_V2/output/test_samples/',\n",
    "\n",
    "    # Split parameters\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "\n",
    "    # Evaluation parameters\n",
    "    'batch_size': 128,\n",
    "    'n_bootstrap': 1000,\n",
    "    'n_timing_runs': 100,\n",
    "    'warmup_runs': 10,\n",
    "\n",
    "    # Device\n",
    "    'device': device,\n",
    "\n",
    "    # Visualization colors\n",
    "    'colors': {\n",
    "        'SVM': '#3498db',\n",
    "        'VGG16': '#e74c3c',\n",
    "        'ResNet50': '#2ecc71',\n",
    "        'RFUAVNet': '#9b59b6'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample sizes for inference comparison and export\n",
    "SAMPLE_SIZES = [20, 100, 500, 1000, 1320]\n",
    "\n",
    "print(f\"Configuration: {CONFIG}\")\n",
    "print(f\"Sample sizes to evaluate/export: {SAMPLE_SIZES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NfOdq2aDWKC"
   },
   "source": [
    "## 4. Helper Functions - Model Definitions\n",
    "\n",
    "All model classes defined once (with `.reshape()` fix for non-contiguous tensors).\n",
    "\n",
    "**Note**: Spectrograms are already RGB (3 channels via Viridis colormap from preprocessing). No grayscale-to-RGB conversion needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8Ke1isNDWKC",
    "outputId": "c80d368b-4dda-4546-e4a4-892bc5fcb5d4"
   },
   "outputs": [],
   "source": [
    "class VGG16FC(nn.Module):\n",
    "    \"\"\"VGG16 with frozen features and trainable classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, from_array: bool = False):\n",
    "        super().__init__()\n",
    "        self.from_array = from_array\n",
    "\n",
    "        vgg = tv_models.vgg16(weights='IMAGENET1K_V1')\n",
    "        self.features = nn.Sequential(*list(vgg.children())[:-1])\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(25088, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.from_array:\n",
    "            x = x.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "        elif x.dim() == 4 and x.shape[-1] == 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class ResNet50FC(nn.Module):\n",
    "    \"\"\"ResNet50 with frozen features and trainable classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = tv_models.resnet50(weights='IMAGENET1K_V1')\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(100352, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4 and x.shape[-1] == 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class RFUAVNet(nn.Module):\n",
    "    \"\"\"RF-UAV-Net: 1D CNN for raw IQ classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # R-unit\n",
    "        self.conv_r = nn.Conv1d(2, 64, kernel_size=5, stride=5)\n",
    "        self.bn_r = nn.BatchNorm1d(64)\n",
    "        self.elu_r = nn.ELU()\n",
    "\n",
    "        # G-units (4x)\n",
    "        self.g_convs = nn.ModuleList([\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, groups=8)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        self.g_bns = nn.ModuleList([nn.BatchNorm1d(64) for _ in range(4)])\n",
    "        self.g_elus = nn.ModuleList([nn.ELU() for _ in range(4)])\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Multi-scale GAP\n",
    "        self.gap1000 = nn.AvgPool1d(1000)\n",
    "        self.gap500 = nn.AvgPool1d(500)\n",
    "        self.gap250 = nn.AvgPool1d(250)\n",
    "        self.gap125 = nn.AvgPool1d(125)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(320, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # R-unit\n",
    "        x = self.elu_r(self.bn_r(self.conv_r(x)))\n",
    "\n",
    "        # G-units with residual connections\n",
    "        g_outputs = []\n",
    "        for i in range(4):\n",
    "            g_out = self.g_elus[i](self.g_bns[i](self.g_convs[i](F.pad(x, (1, 0)))))\n",
    "            g_outputs.append(g_out)\n",
    "            x = g_out + self.pool(x)\n",
    "\n",
    "        # Multi-scale GAP\n",
    "        gaps = [\n",
    "            self.gap1000(g_outputs[0]),\n",
    "            self.gap500(g_outputs[1]),\n",
    "            self.gap250(g_outputs[2]),\n",
    "            self.gap125(g_outputs[3]),\n",
    "            self.gap125(x)\n",
    "        ]\n",
    "\n",
    "        x = torch.cat(gaps, dim=1).flatten(start_dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for RGB spectrograms (already 3 channels from preprocessing).\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx]).float()\n",
    "        y = torch.tensor(self.y[idx]).long()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "print(\"Model classes defined (VGG16FC, ResNet50FC, RFUAVNet, SpectrogramDataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YP6QsIDWKD"
   },
   "source": [
    "## 5. Helper Functions - Split and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHIfN4SoDWKD",
    "outputId": "8a772982-0359-4ab3-e1cd-466b7dfed97c"
   },
   "outputs": [],
   "source": [
    "def get_stratified_file_split(X, y, file_ids, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data at FILE level to prevent data leakage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Features (n_samples, ...)\n",
    "    y : array-like\n",
    "        Labels for stratification (n_samples,)\n",
    "    file_ids : array-like\n",
    "        Source file ID for each sample (n_samples,)\n",
    "    test_size : float\n",
    "        Approximate test set proportion\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_idx, test_idx : arrays\n",
    "        Indices for train/test split\n",
    "    \"\"\"\n",
    "    n_splits = int(1 / test_size)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    train_idx, test_idx = next(sgkf.split(X, y, groups=file_ids))\n",
    "\n",
    "    # Verify no file leakage\n",
    "    train_files = set(file_ids[train_idx])\n",
    "    test_files = set(file_ids[test_idx])\n",
    "    assert len(train_files & test_files) == 0, \"Data leakage detected\"\n",
    "\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def get_sample_indices(y, n_samples, random_state=42):\n",
    "    \"\"\"\n",
    "    Get stratified sample indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        Labels for stratification\n",
    "    n_samples : int or None\n",
    "        Number of samples (None = all)\n",
    "    random_state : int\n",
    "        Random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    indices : array\n",
    "        Sample indices\n",
    "    \"\"\"\n",
    "    if n_samples is None or n_samples >= len(y):\n",
    "        return np.arange(len(y))\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    unique_classes = np.unique(y)\n",
    "    indices = []\n",
    "    samples_per_class = n_samples // len(unique_classes)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        n_take = min(samples_per_class, len(cls_indices))\n",
    "        chosen = np.random.choice(cls_indices, size=n_take, replace=False)\n",
    "        indices.extend(chosen)\n",
    "\n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "def evaluate_pytorch_model(model, dataloader, device, n_timing_runs=100, warmup_runs=10):\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model with per-sample inference timing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Model to evaluate\n",
    "    dataloader : DataLoader\n",
    "        Test data loader\n",
    "    device : torch.device\n",
    "        Device to run on\n",
    "    n_timing_runs : int\n",
    "        Number of timing iterations\n",
    "    warmup_runs : int\n",
    "        Number of warmup iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array\n",
    "        Model predictions\n",
    "    labels : array\n",
    "        True labels\n",
    "    timing_stats : dict\n",
    "        Timing statistics with p50_ms, p95_ms, p99_ms, mean_ms, std_ms\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # First pass: get all predictions\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "\n",
    "    # Get a single sample for timing\n",
    "    sample_x, _ = next(iter(dataloader))\n",
    "    single_sample = sample_x[:1].to(device)\n",
    "\n",
    "    # Warmup runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = model(single_sample)\n",
    "\n",
    "    # Timing runs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_timing_runs):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model(single_sample)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)  # Convert to ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    timing_stats = {\n",
    "        'p50_ms': np.percentile(times, 50),\n",
    "        'p95_ms': np.percentile(times, 95),\n",
    "        'p99_ms': np.percentile(times, 99),\n",
    "        'mean_ms': np.mean(times),\n",
    "        'std_ms': np.std(times)\n",
    "    }\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels), timing_stats\n",
    "\n",
    "\n",
    "def benchmark_svm_model(model, X, n_timing_runs=100, warmup_runs=10):\n",
    "    \"\"\"\n",
    "    Benchmark SVM model with per-sample inference timing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn estimator\n",
    "        Trained SVM model\n",
    "    X : array-like\n",
    "        Test features\n",
    "    n_timing_runs : int\n",
    "        Number of timing iterations\n",
    "    warmup_runs : int\n",
    "        Number of warmup iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    timing_stats : dict\n",
    "        Timing statistics with p50_ms, p95_ms, p99_ms, mean_ms, std_ms\n",
    "    \"\"\"\n",
    "    single_sample = X[:1]\n",
    "\n",
    "    # Warmup runs\n",
    "    for _ in range(warmup_runs):\n",
    "        _ = model.predict(single_sample)\n",
    "\n",
    "    # Timing runs\n",
    "    times = []\n",
    "    for _ in range(n_timing_runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model.predict(single_sample)\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)  # Convert to ms\n",
    "\n",
    "    times = np.array(times)\n",
    "    return {\n",
    "        'p50_ms': np.percentile(times, 50),\n",
    "        'p95_ms': np.percentile(times, 95),\n",
    "        'p99_ms': np.percentile(times, 99),\n",
    "        'mean_ms': np.mean(times),\n",
    "        'std_ms': np.std(times)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_model_size_mb(model_path):\n",
    "    \"\"\"Get model file size in MB.\"\"\"\n",
    "    size_bytes = os.path.getsize(model_path)\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLkTsDqsDWKD"
   },
   "source": [
    "## 6. Load Test Data\n",
    "\n",
    "Load all feature types with file-level split (no data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVkwPMvGDWKD",
    "outputId": "69404185-3fd4-416e-e007-dd5c8225cb60"
   },
   "outputs": [],
   "source": [
    "# Load PSD features\n",
    "data_psd = np.load(CONFIG['psd_path'], allow_pickle=True)\n",
    "\n",
    "X_psd = data_psd['X']\n",
    "y_drone_psd = data_psd['y_drone']\n",
    "y_interference_psd = data_psd['y_interference']\n",
    "file_ids_psd = data_psd['file_ids']\n",
    "drone_classes = data_psd['drone_classes']\n",
    "interference_classes = data_psd['interference_classes']\n",
    "\n",
    "# File-level split\n",
    "_, test_idx_psd = get_stratified_file_split(\n",
    "    X_psd, y_drone_psd, file_ids_psd,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "X_test_psd = X_psd[test_idx_psd]\n",
    "y_test_psd = y_drone_psd[test_idx_psd]\n",
    "y_test_interference_psd = y_interference_psd[test_idx_psd]\n",
    "\n",
    "print(f\"PSD test set: {X_test_psd.shape}\")\n",
    "print(f\"Drone classes: {drone_classes}\")\n",
    "print(f\"Interference classes: {interference_classes}\")\n",
    "\n",
    "# Cleanup\n",
    "del X_psd, y_drone_psd, file_ids_psd, data_psd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwWbv29_DWKD",
    "outputId": "5e1908d0-5c0b-4bba-aa03-f75ac3d18553"
   },
   "outputs": [],
   "source": [
    "# Load spectrogram features (memory-mapped)\n",
    "data_spec = np.load(CONFIG['spectrogram_path'], mmap_mode='r')\n",
    "\n",
    "X_spec = data_spec['X']\n",
    "y_drone_spec = data_spec['y_drone']\n",
    "y_interference_spec = data_spec['y_interference']\n",
    "file_ids_spec = data_spec['file_ids']\n",
    "\n",
    "# File-level split\n",
    "_, test_idx_spec = get_stratified_file_split(\n",
    "    X_spec, y_drone_spec, file_ids_spec,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "X_test_spec = X_spec[test_idx_spec]\n",
    "y_test_spec = y_drone_spec[test_idx_spec]\n",
    "y_test_interference_spec = y_interference_spec[test_idx_spec]\n",
    "\n",
    "print(f\"Spectrogram test set (RGB): {X_test_spec.shape}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset_spec = SpectrogramDataset(X_test_spec, y_test_spec)\n",
    "test_loader_spec = DataLoader(\n",
    "    test_dataset_spec,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Test batches: {len(test_loader_spec)}\")\n",
    "\n",
    "# Cleanup\n",
    "del X_spec, y_drone_spec, file_ids_spec, data_spec, test_idx_spec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCTpThP0DWKE",
    "outputId": "db94727b-6dba-40db-86fb-80d4f8896dda"
   },
   "outputs": [],
   "source": [
    "# Load IQ features (memory-mapped)\n",
    "data_iq = np.load(CONFIG['iq_path'], mmap_mode='r')\n",
    "\n",
    "X_iq = data_iq['X']\n",
    "y_drone_iq = data_iq['y_drone']\n",
    "y_interference_iq = data_iq['y_interference']\n",
    "file_ids_iq = data_iq['file_ids']\n",
    "\n",
    "# File-level split\n",
    "_, test_idx_iq = get_stratified_file_split(\n",
    "    X_iq, y_drone_iq, file_ids_iq,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "X_test_iq = X_iq[test_idx_iq]\n",
    "y_test_iq = y_drone_iq[test_idx_iq]\n",
    "y_test_interference_iq = y_interference_iq[test_idx_iq]\n",
    "\n",
    "print(f\"IQ test set: {X_test_iq.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_iq_t = torch.from_numpy(X_test_iq.copy()).float()\n",
    "y_test_iq_t = torch.from_numpy(y_test_iq.copy()).long()\n",
    "\n",
    "# Create dataloader\n",
    "test_dataset_iq = TensorDataset(X_test_iq_t, y_test_iq_t)\n",
    "test_loader_iq = DataLoader(\n",
    "    test_dataset_iq,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Test batches: {len(test_loader_iq)}\")\n",
    "\n",
    "# Cleanup\n",
    "del X_iq, y_drone_iq, file_ids_iq, data_iq, test_idx_iq\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7IrpamDWKE",
    "outputId": "6599c951-a6cd-468b-fcac-3f7a33e17064"
   },
   "outputs": [],
   "source": [
    "# Verify all test sets have same labels\n",
    "print(f\"PSD test labels: {len(y_test_psd)} samples\")\n",
    "print(f\"Spectrogram test labels: {len(y_test_spec)} samples\")\n",
    "print(f\"IQ test labels: {len(y_test_iq_t)} samples\")\n",
    "\n",
    "# Check label distribution\n",
    "unique, counts = np.unique(y_test_psd, return_counts=True)\n",
    "print(\"\\nTest set class distribution:\")\n",
    "for cls, count in zip(drone_classes, counts):\n",
    "    print(f\"  {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fLWgau_DWKE"
   },
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcUw0-YfDWKE",
    "outputId": "c9851775-585a-46be-a339-bd86259a1d81"
   },
   "outputs": [],
   "source": [
    "# Load and evaluate SVM\n",
    "svm_path = os.path.join(CONFIG['models_dir'], 'svm_psd_drone.pkl')\n",
    "with open(svm_path, 'rb') as f:\n",
    "    svm_data = pickle.load(f)\n",
    "\n",
    "svm_model = svm_data['model']\n",
    "\n",
    "# Full test set predictions\n",
    "print(\"Inferring on full test set...\")\n",
    "svm_preds = svm_model.predict(X_test_psd)\n",
    "\n",
    "svm_acc = accuracy_score(y_test_psd, svm_preds)\n",
    "svm_f1 = f1_score(y_test_psd, svm_preds, average='weighted')\n",
    "\n",
    "print(f\"SVM Test Accuracy: {svm_acc:.4f}\")\n",
    "print(f\"SVM Test F1-Score: {svm_f1:.4f}\")\n",
    "\n",
    "# Benchmark single sample inference\n",
    "print(\"\\nBenchmarking single-sample inference...\")\n",
    "svm_timing = benchmark_svm_model(\n",
    "    svm_model, X_test_psd,\n",
    "    n_timing_runs=CONFIG['n_timing_runs'],\n",
    "    warmup_runs=CONFIG['warmup_runs']\n",
    ")\n",
    "print(f\"Inference time per sample: {svm_timing['p50_ms']:.3f} ms (p50), {svm_timing['p95_ms']:.3f} ms (p95)\")\n",
    "print(f\"SVM Model Size: {get_model_size_mb(svm_path):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wY8oyFJ8DWKE",
    "outputId": "be5c0e9f-5139-4fe6-ca54-3f4814493819"
   },
   "outputs": [],
   "source": [
    "# Load and evaluate VGG16\n",
    "vgg_path = os.path.join(CONFIG['models_dir'], 'vgg16_cnn.pth')\n",
    "vgg_checkpoint = torch.load(vgg_path, map_location=device, weights_only=False)\n",
    "\n",
    "num_classes = len(drone_classes)\n",
    "vgg_model = VGG16FC(num_classes=num_classes)\n",
    "vgg_model.load_state_dict(vgg_checkpoint['model_state_dict'])\n",
    "vgg_model = vgg_model.to(device)\n",
    "\n",
    "vgg_preds, vgg_labels, vgg_timing = evaluate_pytorch_model(\n",
    "    vgg_model, test_loader_spec, device,\n",
    "    n_timing_runs=CONFIG['n_timing_runs'],\n",
    "    warmup_runs=CONFIG['warmup_runs']\n",
    ")\n",
    "\n",
    "vgg_acc = accuracy_score(vgg_labels, vgg_preds)\n",
    "vgg_f1 = f1_score(vgg_labels, vgg_preds, average='weighted')\n",
    "\n",
    "print(f\"VGG16 Test Accuracy: {vgg_acc:.4f}\")\n",
    "print(f\"VGG16 Test F1-Score: {vgg_f1:.4f}\")\n",
    "print(f\"VGG16 Inference Time per sample: {vgg_timing['p50_ms']:.3f} ms (p50), {vgg_timing['p95_ms']:.3f} ms (p95)\")\n",
    "print(f\"VGG16 Model Size: {get_model_size_mb(vgg_path):.2f} MB\")\n",
    "\n",
    "del vgg_model, vgg_checkpoint\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6jhz3X2DWKE",
    "outputId": "e3b6c146-a98d-433b-84be-8a31c735d4ad"
   },
   "outputs": [],
   "source": [
    "# Load and evaluate ResNet50\n",
    "resnet_path = os.path.join(CONFIG['models_dir'], 'resnet50_cnn.pth')\n",
    "resnet_checkpoint = torch.load(resnet_path, map_location=device, weights_only=False)\n",
    "\n",
    "resnet_model = ResNet50FC(num_classes=len(drone_classes))\n",
    "resnet_model.load_state_dict(resnet_checkpoint['model_state_dict'])\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "resnet_preds, resnet_labels, resnet_timing = evaluate_pytorch_model(\n",
    "    resnet_model, test_loader_spec, device,\n",
    "    n_timing_runs=CONFIG['n_timing_runs'],\n",
    "    warmup_runs=CONFIG['warmup_runs']\n",
    ")\n",
    "\n",
    "resnet_acc = accuracy_score(resnet_labels, resnet_preds)\n",
    "resnet_f1 = f1_score(resnet_labels, resnet_preds, average='weighted')\n",
    "\n",
    "print(f\"ResNet50 Test Accuracy: {resnet_acc:.4f}\")\n",
    "print(f\"ResNet50 Test F1-Score: {resnet_f1:.4f}\")\n",
    "print(f\"ResNet50 Inference Time per sample: {resnet_timing['p50_ms']:.3f} ms (p50), {resnet_timing['p95_ms']:.3f} ms (p95)\")\n",
    "print(f\"ResNet50 Model Size: {get_model_size_mb(resnet_path):.2f} MB\")\n",
    "\n",
    "del resnet_model, resnet_checkpoint\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dvi0SCJJDWKE",
    "outputId": "7296e408-b2f6-4cfc-b49a-dc0c78189439"
   },
   "outputs": [],
   "source": [
    "# Load and evaluate RF-UAV-Net\n",
    "rfuavnet_path = os.path.join(CONFIG['models_dir'], 'rfuavnet_iq.pth')\n",
    "rfuavnet_checkpoint = torch.load(rfuavnet_path, map_location=device, weights_only=False)\n",
    "\n",
    "rfuavnet_model = RFUAVNet(num_classes=num_classes)\n",
    "rfuavnet_model.load_state_dict(rfuavnet_checkpoint['model_state_dict'])\n",
    "rfuavnet_model = rfuavnet_model.to(device)\n",
    "\n",
    "rfuavnet_preds, rfuavnet_labels, rfuavnet_timing = evaluate_pytorch_model(\n",
    "    rfuavnet_model, test_loader_iq, device,\n",
    "    n_timing_runs=CONFIG['n_timing_runs'],\n",
    "    warmup_runs=CONFIG['warmup_runs']\n",
    ")\n",
    "\n",
    "rfuavnet_acc = accuracy_score(rfuavnet_labels, rfuavnet_preds)\n",
    "rfuavnet_f1 = f1_score(rfuavnet_labels, rfuavnet_preds, average='weighted')\n",
    "\n",
    "print(f\"RF-UAV-Net Test Accuracy: {rfuavnet_acc:.4f}\")\n",
    "print(f\"RF-UAV-Net Test F1-Score: {rfuavnet_f1:.4f}\")\n",
    "print(f\"RF-UAV-Net Inference Time per sample: {rfuavnet_timing['p50_ms']:.3f} ms (p50), {rfuavnet_timing['p95_ms']:.3f} ms (p95)\")\n",
    "print(f\"RF-UAV-Net Model Size: {get_model_size_mb(rfuavnet_path):.2f} MB\")\n",
    "\n",
    "del rfuavnet_model, rfuavnet_checkpoint\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNZfoFQfDWKE"
   },
   "source": [
    "## 8. Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drn9i0uGDWKE",
    "outputId": "5c553ef6-9d49-4bfb-90a1-e61f03787f75"
   },
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['SVM', 'VGG16', 'ResNet50', 'RFUAVNet'],\n",
    "    'Features': ['PSD', 'Spectrogram', 'Spectrogram', 'Raw IQ'],\n",
    "    'Accuracy': [svm_acc, vgg_acc, resnet_acc, rfuavnet_acc],\n",
    "    'F1-Score': [svm_f1, vgg_f1, resnet_f1, rfuavnet_f1],\n",
    "    'Inference_p50_ms': [\n",
    "        svm_timing['p50_ms'],\n",
    "        vgg_timing['p50_ms'],\n",
    "        resnet_timing['p50_ms'],\n",
    "        rfuavnet_timing['p50_ms']\n",
    "    ],\n",
    "    'Inference_p95_ms': [\n",
    "        svm_timing['p95_ms'],\n",
    "        vgg_timing['p95_ms'],\n",
    "        resnet_timing['p95_ms'],\n",
    "        rfuavnet_timing['p95_ms']\n",
    "    ],\n",
    "    'Inference_p99_ms': [\n",
    "        svm_timing['p99_ms'],\n",
    "        vgg_timing['p99_ms'],\n",
    "        resnet_timing['p99_ms'],\n",
    "        rfuavnet_timing['p99_ms']\n",
    "    ],\n",
    "    'Model_Size_MB': [\n",
    "        get_model_size_mb(svm_path),\n",
    "        get_model_size_mb(vgg_path),\n",
    "        get_model_size_mb(resnet_path),\n",
    "        get_model_size_mb(rfuavnet_path)\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Model Comparison Results ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Store predictions for statistical tests\n",
    "predictions = {\n",
    "    'SVM': svm_preds,\n",
    "    'VGG16': vgg_preds,\n",
    "    'ResNet50': resnet_preds,\n",
    "    'RFUAVNet': rfuavnet_preds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoKrVVCODWKF"
   },
   "source": [
    "## 10. Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FX2QkcfDWKF",
    "outputId": "e3444f10-094b-43c0-c545-0e1cb93e2924"
   },
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "per_class_metrics = {}\n",
    "\n",
    "for model_name, preds in predictions.items():\n",
    "    labels = y_test_psd\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, preds, labels=range(len(drone_classes)), zero_division=0\n",
    "    )\n",
    "    per_class_metrics[model_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "# Print per-class F1 for best model\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "print(f\"\\n=== {best_model} Per-Class Performance ===\")\n",
    "print(f\"{'Class':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, cls in enumerate(drone_classes):\n",
    "    metrics = per_class_metrics[best_model]\n",
    "    print(f\"{cls:<10} {metrics['precision'][i]:<12.4f} {metrics['recall'][i]:<12.4f} \"\n",
    "          f\"{metrics['f1'][i]:<12.4f} {metrics['support'][i]:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8M9-5yLDWKF"
   },
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "4ka6AuLEDWKF",
    "outputId": "36a8a84f-677b-4871-9675-d4f3e143a99a"
   },
   "outputs": [],
   "source": [
    "# Accuracy & F1 Comparison with Plotly\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Model Accuracy Comparison', 'Model F1-Score Comparison'))\n",
    "\n",
    "models = results_df['Model'].values\n",
    "colors = [CONFIG['colors'][m] for m in models]\n",
    "\n",
    "# Accuracy bars\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(models), y=results_df['Accuracy'].values,\n",
    "    marker_color=colors, text=[f\"{v:.4f}\" for v in results_df['Accuracy'].values],\n",
    "    textposition='outside', name='Accuracy'\n",
    "), row=1, col=1)\n",
    "\n",
    "# F1-Score bars\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(models), y=results_df['F1-Score'].values,\n",
    "    marker_color=colors, text=[f\"{v:.4f}\" for v in results_df['F1-Score'].values],\n",
    "    textposition='outside', name='F1-Score', showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison - Accuracy and F1-Score',\n",
    "    height=500, width=1200\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1.1], title_text='Accuracy', row=1, col=1)\n",
    "fig.update_yaxes(range=[0, 1.1], title_text='F1-Score (Weighted)', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bzmV9e1-DWKF",
    "outputId": "c2884889-ddef-4942-c5e9-cbc55580fc1c"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrices with Plotly\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=['SVM', 'VGG16', 'ResNet50', 'RFUAVNet'],\n",
    "                    horizontal_spacing=0.12, vertical_spacing=0.12)\n",
    "\n",
    "colorscales = ['Blues', 'Reds', 'Greens', 'Purples']\n",
    "model_names = ['SVM', 'VGG16', 'ResNet50', 'RFUAVNet']\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "for idx, (model_name, (row, col)) in enumerate(zip(model_names, positions)):\n",
    "    cm = confusion_matrix(y_test_psd, predictions[model_name])\n",
    "    acc = results_df[results_df['Model'] == model_name]['Accuracy'].values[0]\n",
    "\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=cm, x=list(drone_classes), y=list(drone_classes),\n",
    "        colorscale=colorscales[idx], text=cm, texttemplate='%{text}',\n",
    "        textfont={'size': 10}, showscale=False, hoverongaps=False\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='All Models Confusion Matrices',\n",
    "    height=900, width=1000\n",
    ")\n",
    "\n",
    "# Update axes for all subplots\n",
    "for i in range(1, 3):\n",
    "    for j in range(1, 3):\n",
    "        fig.update_xaxes(title_text='Predicted', row=i, col=j)\n",
    "        fig.update_yaxes(title_text='True', autorange='reversed', row=i, col=j)\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "LkaoFwVvDWKF",
    "outputId": "fc5606f5-3ea6-4880-8701-5fd903efac13"
   },
   "outputs": [],
   "source": [
    "# Per-Class F1 Scores with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name, metrics in per_class_metrics.items():\n",
    "    color = CONFIG['colors'][model_name]\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(drone_classes), y=metrics['f1'],\n",
    "        name=model_name, marker_color=color, opacity=0.8\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Per-Class F1-Score Comparison',\n",
    "    xaxis_title='Class',\n",
    "    yaxis_title='F1-Score',\n",
    "    yaxis_range=[0, 1.1],\n",
    "    barmode='group',\n",
    "    height=600, width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "35OHlQ4NDWKF",
    "outputId": "728b7073-4497-4c63-b897-cc5fee6006f0"
   },
   "outputs": [],
   "source": [
    "# Inference Time Comparison with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "models = results_df['Model'].values\n",
    "p50_times = results_df['Inference_p50_ms'].values\n",
    "p95_times = results_df['Inference_p95_ms'].values\n",
    "colors_list = [CONFIG['colors'][m] for m in models]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=list(models), x=p50_times, orientation='h',\n",
    "    marker_color=colors_list, name='p50 (median)',\n",
    "    text=[f\"p50: {t:.2f}ms\" for t in p50_times], textposition='outside'\n",
    "))\n",
    "\n",
    "# Add p95 as error bars\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=list(models), x=p95_times, mode='markers',\n",
    "    marker=dict(symbol='line-ns', size=15, color='gray', line_width=2),\n",
    "    name='p95'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Inference Time Comparison - Single Sample (p50 with p95 markers)',\n",
    "    xaxis_title='Inference Time per Sample (ms)',\n",
    "    height=500, width=900,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "M7RLYdF5DWKF",
    "outputId": "f9f7fd62-5d89-413c-d3e3-1a733e69292d"
   },
   "outputs": [],
   "source": [
    "# Model Size Comparison with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "models = results_df['Model'].values\n",
    "sizes = results_df['Model_Size_MB'].values\n",
    "colors_list = [CONFIG['colors'][m] for m in models]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=list(models), x=sizes, orientation='h',\n",
    "    marker_color=colors_list,\n",
    "    text=[f\"{s:.2f} MB\" for s in sizes], textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Size Comparison',\n",
    "    xaxis_title='Model Size (MB)',\n",
    "    height=500, width=900\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "tK7AULAmDWKF",
    "outputId": "4bea6375-82d6-4bce-a355-458998539833"
   },
   "outputs": [],
   "source": [
    "# Radar Chart with Plotly\n",
    "def normalize(values):\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    if max_val == min_val:\n",
    "        return np.ones_like(values)\n",
    "    return (values - min_val) / (max_val - min_val)\n",
    "\n",
    "accuracy_norm = results_df['Accuracy'].values\n",
    "f1_norm = results_df['F1-Score'].values\n",
    "time_norm = 1 - normalize(results_df['Inference_p50_ms'].values)\n",
    "size_norm = 1 - normalize(results_df['Model_Size_MB'].values)\n",
    "\n",
    "categories = ['Accuracy', 'F1-Score', 'Speed (inverse time)', 'Compactness (inverse size)']\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    values = [accuracy_norm[idx], f1_norm[idx], time_norm[idx], size_norm[idx]]\n",
    "    # Close the polygon\n",
    "    values.append(values[0])\n",
    "    categories_closed = categories + [categories[0]]\n",
    "\n",
    "    color = CONFIG['colors'][model]\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values, theta=categories_closed,\n",
    "        fill='toself', name=model,\n",
    "        line_color=color, fillcolor=color, opacity=0.3\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Multi-Metric Model Comparison (Normalized)',\n",
    "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "    height=700, width=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "save_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GxZ-FY9DWKF"
   },
   "source": [
    "## 12. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQVwYArGDWKI",
    "outputId": "93ea220a-7fab-4acb-8d53-dc19823cab60"
   },
   "outputs": [],
   "source": [
    "# McNemar's Test\n",
    "def mcnemar_test(y_true, pred1, pred2):\n",
    "    \"\"\"McNemar's test for paired predictions.\"\"\"\n",
    "    correct1 = (pred1 == y_true)\n",
    "    correct2 = (pred2 == y_true)\n",
    "\n",
    "    n01 = np.sum(~correct1 & correct2)\n",
    "    n10 = np.sum(correct1 & ~correct2)\n",
    "\n",
    "    if n01 + n10 == 0:\n",
    "        return 1.0, 0.0\n",
    "\n",
    "    statistic = (abs(n01 - n10) - 1)**2 / (n01 + n10)\n",
    "    p_value = 1 - chi2.cdf(statistic, df=1)\n",
    "\n",
    "    return p_value, statistic\n",
    "\n",
    "model_names = ['SVM', 'VGG16', 'ResNet50', 'RFUAVNet']\n",
    "mcnemar_results = []\n",
    "\n",
    "print(\"=== McNemar's Test (Pairwise) ===\")\n",
    "print(f\"{'Model 1':<12} {'Model 2':<12} {'p-value':<12} {'Statistic':<12} {'Significant'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        model1 = model_names[i]\n",
    "        model2 = model_names[j]\n",
    "\n",
    "        p_value, stat = mcnemar_test(\n",
    "            y_test_psd,\n",
    "            predictions[model1],\n",
    "            predictions[model2]\n",
    "        )\n",
    "\n",
    "        significant = \"Yes (p<0.05)\" if p_value < 0.05 else \"No\"\n",
    "        print(f\"{model1:<12} {model2:<12} {p_value:<12.4f} {stat:<12.4f} {significant}\")\n",
    "\n",
    "        mcnemar_results.append({\n",
    "            'Model1': model1,\n",
    "            'Model2': model2,\n",
    "            'p_value': p_value,\n",
    "            'statistic': stat,\n",
    "            'significant': p_value < 0.05\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTIYCwQHDWKI",
    "outputId": "a23625b0-0445-434d-ff05-95e5a0c2a1c0"
   },
   "outputs": [],
   "source": [
    "# Bootstrap Confidence Intervals\n",
    "def bootstrap_ci(y_true, y_pred, n_iterations=1000, ci=0.95, random_state=42):\n",
    "    \"\"\"Bootstrap confidence interval for accuracy.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = len(y_true)\n",
    "    accuracies = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        accuracies.append(accuracy_score(y_true_boot, y_pred_boot))\n",
    "\n",
    "    alpha = 1 - ci\n",
    "    lower = np.percentile(accuracies, 100 * alpha / 2)\n",
    "    upper = np.percentile(accuracies, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return lower, upper\n",
    "\n",
    "print(\"\\n=== Bootstrap 95% Confidence Intervals (Accuracy) ===\")\n",
    "print(f\"{'Model':<12} {'Accuracy':<12} {'CI Lower':<12} {'CI Upper':<12} {'CI Width'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name in model_names:\n",
    "    acc = accuracy_score(y_test_psd, predictions[model_name])\n",
    "    lower, upper = bootstrap_ci(\n",
    "        y_test_psd,\n",
    "        predictions[model_name],\n",
    "        n_iterations=CONFIG['n_bootstrap'],\n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    width = upper - lower\n",
    "    print(f\"{model_name:<12} {acc:<12.4f} {lower:<12.4f} {upper:<12.4f} {width:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaWtWCKnDWKI",
    "outputId": "afacecaa-1444-48aa-b3ed-9e464c9264fd"
   },
   "outputs": [],
   "source": [
    "# Cohen's Kappa\n",
    "print(\"\\n=== Cohen's Kappa (Agreement beyond chance) ===\")\n",
    "print(f\"{'Model':<12} {'Kappa':<12} {'Interpretation'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def interpret_kappa(kappa):\n",
    "    if kappa < 0:\n",
    "        return \"Poor (worse than random)\"\n",
    "    elif kappa < 0.20:\n",
    "        return \"Slight\"\n",
    "    elif kappa < 0.40:\n",
    "        return \"Fair\"\n",
    "    elif kappa < 0.60:\n",
    "        return \"Moderate\"\n",
    "    elif kappa < 0.80:\n",
    "        return \"Substantial\"\n",
    "    else:\n",
    "        return \"Almost perfect\"\n",
    "\n",
    "for model_name in model_names:\n",
    "    kappa = cohen_kappa_score(y_test_psd, predictions[model_name])\n",
    "    interpretation = interpret_kappa(kappa)\n",
    "    print(f\"{model_name:<12} {kappa:<12.4f} {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5Gf0NRyDWKI"
   },
   "source": [
    "## 13. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_oHrpy3DWKI",
    "outputId": "90fa9928-4493-4c33-83e9-87b40d0e3ca6"
   },
   "outputs": [],
   "source": [
    "# Misclassification Patterns\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_preds = predictions[best_model]\n",
    "\n",
    "misclassified_mask = (best_preds != y_test_psd)\n",
    "misclassified_true = y_test_psd[misclassified_mask]\n",
    "misclassified_pred = best_preds[misclassified_mask]\n",
    "\n",
    "print(f\"\\n=== {best_model} Error Analysis ===\")\n",
    "print(f\"Total test samples: {len(y_test_psd)}\")\n",
    "print(f\"Misclassified samples: {np.sum(misclassified_mask)} ({100*np.sum(misclassified_mask)/len(y_test_psd):.2f}%)\")\n",
    "\n",
    "print(\"\\nMost common misclassification pairs (True -> Predicted):\")\n",
    "misclass_pairs = list(zip(misclassified_true, misclassified_pred))\n",
    "unique_pairs, counts = np.unique(misclass_pairs, axis=0, return_counts=True)\n",
    "sorted_indices = np.argsort(-counts)[:10]\n",
    "\n",
    "print(\"  TRUE -> PRED\")\n",
    "for idx in sorted_indices:\n",
    "    true_cls = drone_classes[unique_pairs[idx][0]]\n",
    "    pred_cls = drone_classes[unique_pairs[idx][1]]\n",
    "    count = counts[idx]\n",
    "    pct = 100 * count / np.sum(misclassified_mask)\n",
    "    print(f\"  {true_cls} -> {pred_cls}: {count} errors ({pct:.1f}% of errors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06_MOr8-DWKJ",
    "outputId": "83b824dd-6a4a-45da-bcae-dff5cc327d32"
   },
   "outputs": [],
   "source": [
    "# Model Agreement Analysis\n",
    "pred_matrix = np.array([predictions[m] for m in model_names]).T\n",
    "\n",
    "all_agree = np.all(pred_matrix == pred_matrix[:, 0:1], axis=1)\n",
    "all_correct = np.all(pred_matrix == y_test_psd[:, None], axis=1)\n",
    "all_wrong = np.all(pred_matrix != y_test_psd[:, None], axis=1)\n",
    "\n",
    "print(\"\\n=== Model Agreement Analysis ===\")\n",
    "print(f\"Samples where all models agree: {np.sum(all_agree)} ({100*np.sum(all_agree)/len(y_test_psd):.2f}%)\")\n",
    "print(f\"Samples where all models are correct: {np.sum(all_correct)} ({100*np.sum(all_correct)/len(y_test_psd):.2f}%)\")\n",
    "print(f\"Samples where all models are wrong: {np.sum(all_wrong)} ({100*np.sum(all_wrong)/len(y_test_psd):.2f}%)\")\n",
    "\n",
    "no_model_correct = np.all(pred_matrix != y_test_psd[:, None], axis=1)\n",
    "print(f\"\\nHard samples (no model correct): {np.sum(no_model_correct)}\")\n",
    "if np.sum(no_model_correct) > 0:\n",
    "    hard_true = y_test_psd[no_model_correct]\n",
    "    unique_hard, counts_hard = np.unique(hard_true, return_counts=True)\n",
    "    print(\"Distribution by class:\")\n",
    "    for cls_idx, count in zip(unique_hard, counts_hard):\n",
    "        print(f\"  {drone_classes[cls_idx]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9jrgderDWKJ"
   },
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adrVDOgQDWKJ",
    "outputId": "086f5898-8982-4bcc-dbb1-4dcd36c13a85"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nRANKING (by Accuracy):\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"  {idx+1}. {row['Model']:<10} Acc={row['Accuracy']:.4f} F1={row['F1-Score']:.4f}\")\n",
    "\n",
    "best = results_df.iloc[0]\n",
    "fastest = results_df.loc[results_df['Inference_p50_ms'].idxmin()]\n",
    "smallest = results_df.loc[results_df['Model_Size_MB'].idxmin()]\n",
    "\n",
    "print(f\"\\nBest accuracy: {best['Model']} ({best['Accuracy']:.4f})\")\n",
    "print(f\"Fastest inference: {fastest['Model']} (p50={fastest['Inference_p50_ms']:.2f}ms)\")\n",
    "print(f\"Smallest model: {smallest['Model']} ({smallest['Model_Size_MB']:.2f}MB)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUgsPsXDWKJ"
   },
   "source": [
    "## 15. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqElGyLiDWKJ",
    "outputId": "c4bbc83c-6b17-43a2-89b8-aa5edfee09b6"
   },
   "outputs": [],
   "source": [
    "# Save results DataFrame\n",
    "results_path = os.path.join(CONFIG['models_dir'], 'model_comparison_results.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Results saved to {results_path}\")\n",
    "\n",
    "# Save detailed comparison\n",
    "comparison_dict = {\n",
    "    'results_df': results_df,\n",
    "    'per_class_metrics': per_class_metrics,\n",
    "    'mcnemar_results': mcnemar_results,\n",
    "    'predictions': predictions,\n",
    "    'true_labels': y_test_psd,\n",
    "    'drone_classes': drone_classes,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "comparison_path = os.path.join(CONFIG['models_dir'], 'model_comparison_full.pkl')\n",
    "with open(comparison_path, 'wb') as f:\n",
    "    pickle.dump(comparison_dict, f)\n",
    "print(f\"Full comparison saved to {comparison_path}\")\n",
    "\n",
    "# Save export summary\n",
    "if 'export_df' in globals():\n",
    "    export_summary_path = os.path.join(CONFIG['sample_export_dir'], 'export_summary.csv')\n",
    "    export_df.to_csv(export_summary_path, index=False)\n",
    "    print(f\"Export summary saved to {export_summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
